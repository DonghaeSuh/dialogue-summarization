{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from copy import deepcopy\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_utterance(data:json):\n",
    "    \"\"\"\n",
    "    Remove empty utterances from the data\n",
    "    \"\"\"\n",
    "    for example in data:\n",
    "        example['input']['conversation'] = [cvt for cvt in example['input']['conversation'] if cvt['utterance'].strip() not in [\"\",\".\"]]\n",
    "    return data\n",
    "# 최종 단계에서 if utterance = \"\" 하면 되는 거여서 굳이 함수로 만들 필요 없음\n",
    "\n",
    "def correct_wrong_output(data:json, path:str):\n",
    "    \"\"\"\n",
    "    1. Correct wrong speakers in outputs of train samples 'train-000401', 'train-000402, 'train-000111'\n",
    "    2. Add dot(.) at the end of the last sentence in outputs of train samples 'train-000130'\n",
    "    4. Replace speaker name 'SSD' with 'SD' in outputso of 'train-000030', 'train-000193' and 'dev-000085'\n",
    "    5. Remove duplicate sentences in outputs of dev samples 'dev-000093'.\n",
    "    \"\"\"\n",
    "    if 'train' in path:\n",
    "        # Correct wrong speakers\n",
    "        data[400]['output'] = data[400]['output'].replace('SD2100504','SD2110504')\n",
    "        data[401]['output'] = data[401]['output'].replace('SD2110503','SD2100503')\n",
    "        data[110]['output'] = data[110]['output'].replace('SD20010813','SD2001083')\n",
    "        # Add dot(.) at the end of the last sentence\n",
    "        data[129]['output'] = data[129]['output'] + '.'\n",
    "        # Replace speaker name\n",
    "        data[29]['output'] = data[29]['output'].replace('SSD', 'SD')\n",
    "        data[192]['output'] = data[192]['output'].replace('SSD', 'SD')\n",
    "\n",
    "    elif 'dev' in path:\n",
    "        # Replace speaker name\n",
    "        data[84]['output'] = data[84]['output'].replace('SSD', 'SD')\n",
    "        # Remove duplicate sentences\n",
    "        data[92]['output'] = '.'.join(data[92]['output'].split('.')[1:]).strip()\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def add_space_after_period_and_remove_control_characters(data:json, path:str):\n",
    "    \"\"\"\n",
    "    Add space after period if there is no space after period\n",
    "    text = re.sub(r'\\.(?=\\S)', '. ', text)\n",
    "    \"\"\"\n",
    "    if 'train' or 'dev' in path:\n",
    "        # Add space after period in utterances\n",
    "        for example in data:\n",
    "            example['input']['conversation'] = [{'speaker': cvt['speaker'], 'utterance': re.sub(r'\\.(?=\\S)', '. ', cvt['utterance']).strip()} for cvt in example['input']['conversation']]\n",
    "\n",
    "        # Remove_control_characters and Add space after period in outputs\n",
    "        for example in data:\n",
    "            output = re.sub(r'[\\x00-\\x1F\\x7F-\\x9F]', '', example['output'])\n",
    "            example['output'] = re.sub(r'\\.(?=\\S)', '. ', output).strip()\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def change_weird_output(data:json, path:str):\n",
    "    \"\"\"\n",
    "    Standardize the type of the output of train-000032, train-000418, dev-000074, dev-000093\n",
    "    \"\"\"\n",
    "    # Standardize the type of outputs\n",
    "    if 'train' in path:\n",
    "        # train-000032 : total_summary 교체\n",
    "        output = data[31]['output'].split('.')\n",
    "        total_summary = \"두 화자는 이 대화에서 진로 관련 고민에 대해 이야기했습니다. \"\n",
    "        data[31]['output'] = total_summary + '.'.join(output[1:])\n",
    "\n",
    "        # train-000418 : total_summary 추가\n",
    "        total_summary = \"두 화자는 이 대화에서 다이어트에 대해 이야기했습니다. \"\n",
    "        data[417]['output'] = total_summary + data[417]['output']\n",
    "\n",
    "\n",
    "    elif 'dev' in path:\n",
    "        # dev-000074 : total_summary 수정\n",
    "        data[73]['output'] = \"두 화자는 \"+ data[73]['output'] # 이 대화에서 -> 두 화자는 이 대화에서\n",
    "\n",
    "        # dev-000093 : total_summary 추가\n",
    "        total_summary = \"두 화자는 이 대화에서 엔시티와 방탄소년단에 대해 이야기 했습니다. \"\n",
    "        data[92]['output'] = total_summary + data[92]['output']\n",
    "    \n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def remove_sd_in_total_summary(data:json, path:str):\n",
    "    \"\"\"\n",
    "    Remove 'SD' in total_summary of train-000020 and train-000176\n",
    "    \"\"\"\n",
    "    if 'train' in path:\n",
    "        # train-000020 : total_summary 수정\n",
    "        data[19]['output'] = data[19]['output'].replace('SD2000039의 꿈인 ','')\n",
    "\n",
    "        # train-000176 : total_summary '.' 가 빠져있던 것을 수정\n",
    "        output = data[175]['output']\n",
    "        data[175]['output'] = re.sub(r'(장단점에 대해 말했습니다)\\s+(SD\\d{7}(?:은|는))', r'\\1. \\2', output)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def total_summary_generalization(data:json, path:str):\n",
    "    \"\"\"\n",
    "    Standardize the format of the total summary in the first sentence of the output \n",
    "    to start with \"두 화자는 이 대화에서\".\n",
    "    \"\"\"\n",
    "    types = [\"두 화자는\", \"화자들은\" ,\"두 사람은\", \"이 대화에서는\"] # \"두 화자는 이 대화에서\"\n",
    "    types2 = r\"SD\\d{7}(?:와|과).*SD\\d{7}(?:은|는)\"\n",
    "\n",
    "    if 'train' or 'dev' in path:\n",
    "        for example in data:\n",
    "            output = example['output']\n",
    "            total_summary = output.split('.')[0]\n",
    "\n",
    "            if \"두 화자는 이 대화에서\" in total_summary:\n",
    "                continue\n",
    "            elif re.search(types2, total_summary):\n",
    "                total_summary = re.sub(r'(.*)'+types2, '두 화자는 이 대화에서', total_summary)+'.'\n",
    "                example['output'] = total_summary+'.'.join(output.split('.')[1:])\n",
    "            else:\n",
    "                for type in types:\n",
    "                    if type in total_summary:\n",
    "                        total_summary = re.sub(r'(.*)'+type, '두 화자는 이 대화에서', total_summary)+'.'\n",
    "                        example['output'] = total_summary+'.'.join(output.split('.')[1:])\n",
    "                        break\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def remove_duplicate_output_words(data:json, path:str):\n",
    "    \"\"\"\n",
    "    Remove duplicate words in outputs of train samples \n",
    "    (그리고 그리고) 'train-000387', \n",
    "    (대화에서 대화에서) 'train-000383', 'train-000451', 'train-000479', 'train-000495'\n",
    "    (좋은 좋은) 'train-000268'\n",
    "    (화자 화자) 'train-000092', 'train-000231'\n",
    "    (할머니가 할머니가) 'train-000128'\n",
    "    (가도 가도) 'train-000338'\n",
    "    \"\"\"\n",
    "    if 'train' in path:\n",
    "        # Remove duplicate words\n",
    "        ids = [387, 383, 451, 479, 495, 268, 92, 231, 128, 338]\n",
    "        for id in ids:\n",
    "            output = data[id-1]['output']\n",
    "            output = re.sub(r'\\b(\\w+)\\b(?:\\s+\\1\\b)+', r'\\1', output)\n",
    "            data[id-1]['output'] = output\n",
    "\n",
    "    return data\n",
    "\n",
    "# stopword로 제거하기 전, 예외적인 경우 처리\n",
    "def remove_stopwords_exception(data:json, path:str):\n",
    "    \"\"\"\n",
    "    manual exception handling for removing stopwords in utterances\n",
    "    (\" 좋 \") : train과 dev에서는 의미없게 단어 사이에 추가된 단어이지만, test에서는 의미있는 단어로 사용되는 경우(좋 은데, 좋 을 것)가 있음\n",
    "        ex) 'test-000119' : \"좋 은데\" -> \"좋은데\"\n",
    "            'test-000303' : \"좋 을 것\" -> \"좋을 것\"\n",
    "            'test-000348' : \"좋 다고\" -> \"좋다고\"\n",
    "    \"\"\"\n",
    "    if 'test' in path:\n",
    "        # \" 좋 \" -> \" 좋\"\n",
    "        data[118]['input']['conversation'][-1]['utterance'] = data[118]['input']['conversation'][-1]['utterance'].replace(' 좋 ', ' 좋')\n",
    "        data[302]['input']['conversation'][-2]['utterance'] = data[302]['input']['conversation'][-2]['utterance'].replace(' 좋 ', ' 좋')\n",
    "        data[347]['input']['conversation'][4]['utterance'] = data[347]['input']['conversation'][4]['utterance'].replace(' 좋 ', ' 좋')\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# SD\\d{7} 앞에 '화자' 제거\n",
    "def remove_hwaja_before_speaker_in_output(data:json, path:str):\n",
    "    \"\"\"\n",
    "    Remove '화자' before 'SD\\d{7}' in outputs of train samples\n",
    "    \"\"\"\n",
    "    if 'train' in path:\n",
    "        for example in data:\n",
    "            output = example['output']\n",
    "            output = re.sub(r'화자\\s*(SD\\d{7})', r'\\1', output)\n",
    "            example['output'] = output\n",
    "\n",
    "    return data\n",
    "\n",
    "# SD\\d{7} 뒤에 아무런 조사가 붙지 않은 경우 수정\n",
    "def add_josa_after_speaker_in_output(data:json, path:str):\n",
    "    \"\"\"\n",
    "    <Train>\n",
    "    - train-243 : SD2002060 또한 -> SD2002060도\n",
    "    - train-410 : 또 SD2100516 자신은 -> 또 자신은\n",
    "    - train-441 :  SD2110545 유기견을 -> 또 유기견을 / 또 SD2100546은 -> SD2100546은\n",
    "    - train-495 :  SD2100589에도 -> SD2100589에게도 / SD2100589 헬스장 -> SD2100589에게 헬스장\n",
    "    \"\"\"\n",
    "    if 'train' in path:\n",
    "        data[242]['output'] = data[242]['output'].replace('SD2002060 또한', 'SD2002060도')\n",
    "        data[409]['output'] = data[409]['output'].replace('또 SD2100516 자신은', '또 자신은')\n",
    "        data[440]['output'] = data[440]['output'].replace('SD2110545 유기견을', '또 유기견을').replace('또 SD2100546은', 'SD2100546은')\n",
    "        data[494]['output'] = data[494]['output'].replace('SD2100589에도', 'SD2100589에게도').replace('SD2100589 헬스장', 'SD2100589에게 헬스장')\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "# speaker summary generalization\n",
    "def speaker_summary_generalization(data:json, path:str):\n",
    "    \"\"\"\n",
    "    Standardize the format of the speaker summary in the first sentence of the output \n",
    "    to start with \"SD\\d{7}은(는)\".\n",
    "    \"\"\"\n",
    "    if 'train' in path:\n",
    "        # exception handling \n",
    "        # train-000496 \"SD2100589가\" -> \"SD2100589는\"\n",
    "        # train-000476 \"SD2100573도\" -> \"SD2100573은\"\n",
    "        data[495]['output'] = data[495]['output'].replace('SD2100589가', 'SD2100589는')\n",
    "        data[475]['output'] = data[475]['output'].replace('SD2100573도', 'SD2100573은')\n",
    "    \n",
    "\n",
    "    def check_first_speaker_and_first_summary_speaker_is_same(example:json) -> bool:\n",
    "        \"\"\"\n",
    "        Check if the first speaker and the first speaker summary speaker are the same.\n",
    "        \"\"\"\n",
    "        first_speaker = example['input']['conversation'][0]['speaker']\n",
    "        first_summary_speaker = re.search(r'SD\\d{7}', example['output']).group()\n",
    "        return first_speaker == first_summary_speaker\n",
    "\n",
    "\n",
    "    def find_split_indexes(text: str) -> list[tuple]:\n",
    "        \"\"\"\n",
    "        Find the indexes(strat, end) to split the structured summary.\n",
    "        \"\"\"\n",
    "        # The number of 'SD{7}[은는]{1}'\n",
    "        num_speakers = len(re.findall(r'SD\\d{7}[은는]{1}', text))\n",
    "\n",
    "        # Split the structured summary based on the number of 'SD{7}[은는]{1}'\n",
    "        if num_speakers == 2: \n",
    "            mathes = re.finditer(r'SD\\d{7}[은는]{1}', text)\n",
    "            return [(match.group(), match.start()) for match in mathes] # [(speaker1, start_id_1), (speaker2, start_id_2)]\n",
    "        \n",
    "        elif num_speakers == 0:\n",
    "            matches = re.finditer(r'SD\\d{7}\\w+', text)\n",
    "\n",
    "            first_match = next(matches)\n",
    "            first_tuple = (first_match.start(), first_match.group())\n",
    "\n",
    "            for match in matches:\n",
    "                if match.group()[:9] == first_tuple[1][:9]: # SD{7}가 같은 경우\n",
    "                    continue\n",
    "                return [(first_tuple[1], first_tuple[0]), (match.group(), match.start())]\n",
    "            \n",
    "        elif num_speakers == 1:\n",
    "            matches = re.finditer(r'SD\\d{7}\\w+', text)\n",
    "\n",
    "            first_match = next(matches)\n",
    "            first_tuple = (first_match.start(), first_match.group())\n",
    "\n",
    "            for match in matches:\n",
    "                if match.group()[:9] == first_tuple[1][:9]: # SD{7}가 같은 경우\n",
    "                    continue\n",
    "                return [(first_tuple[1], first_tuple[0]), (match.group(), match.start())]\n",
    "            \n",
    "        elif num_speakers == 3:\n",
    "            matches = re.finditer(r'SD\\d{7}[은는]{1}', text)\n",
    "\n",
    "            first_match = next(matches)\n",
    "            first_tuple = (first_match.start(), first_match.group())\n",
    "\n",
    "            for match in matches:\n",
    "                if match.group()[:9] == first_tuple[1][:9]: # SD{7}가 같은 경우\n",
    "                    continue\n",
    "                return [(first_tuple[1], first_tuple[0]), (match.group(), match.start())]\n",
    "        \n",
    "        elif num_speakers == 4:\n",
    "            matches = re.finditer(r'SD\\d{7}[은는]{1}', text)\n",
    "\n",
    "            first_match = next(matches)\n",
    "            first_tuple = (first_match.start(), first_match.group())\n",
    "\n",
    "            for match in matches:\n",
    "                if match.group()[:9] == first_tuple[1][:9]: # SD{7}가 같은 경우\n",
    "                    continue\n",
    "                return [(first_tuple[1], first_tuple[0]), (match.group(), match.start())]\n",
    "\n",
    "    if 'test' in path:\n",
    "        for example in data:\n",
    "            # Find speaker_1 and speaker_2\n",
    "            speaker_1 = example['input']['conversation'][0]['speaker']\n",
    "\n",
    "            for speaker in example['input']['conversation']:\n",
    "                if speaker['speaker'] != speaker_1:\n",
    "                    speaker_2 = speaker['speaker']\n",
    "                    break\n",
    "                \n",
    "            example['input']['speaker_1'] = speaker_1\n",
    "            example['input']['speaker_2'] = speaker_2\n",
    "\n",
    "    elif 'train' or 'dev' in path:\n",
    "        for example in data:\n",
    "            output = example['output']\n",
    "\n",
    "            # Find the indexes to split the structured summary\n",
    "            split_indexes = find_split_indexes(output) # [(r'speaker1\\w+', start_id_1), (r'speaker2\\w+', start_id_2)]\n",
    "            speaker_1, speaker_2 = split_indexes[0][0][:9], split_indexes[1][0][:9] # SD{7}\n",
    "\n",
    "            # Split the structured summary\n",
    "            total_summary = output[:split_indexes[0][1]].strip()\n",
    "            if check_first_speaker_and_first_summary_speaker_is_same(example):\n",
    "                # The first speaker and the first speaker summary speaker are the same\n",
    "                example['input']['speaker_1'] = speaker_1\n",
    "                example['input']['speaker_2'] = speaker_2\n",
    "\n",
    "                speaker_1_summary = output[split_indexes[0][1]:split_indexes[1][1]].strip()\n",
    "                speaker_2_summary = output[split_indexes[1][1]:].strip()\n",
    "            else:\n",
    "                # The first speaker and the first speaker summary speaker are different\n",
    "                speaker_1, speaker_2 = speaker_2, speaker_1 # Swap the speakers\n",
    "                example['input']['speaker_1'] = speaker_1\n",
    "                example['input']['speaker_2'] = speaker_2\n",
    "\n",
    "                speaker_1_summary = output[split_indexes[1][1]:].strip()\n",
    "                speaker_2_summary = output[split_indexes[0][1]:split_indexes[1][1]].strip()\n",
    "\n",
    "            # Standardize the format of the speaker summary\n",
    "            output_format = f'''## 전반적인 요약\\n{total_summary}\\n\\n## {speaker_1} 요약\\n{speaker_1_summary}\\n\\n## {speaker_2} 요약\\n{speaker_2_summary}'''\n",
    "            \n",
    "            example['output'] = output_format\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "\n",
    "    stopwords_pattern = [r'\\w~', r'\\b으\\b', r'\\b그\\b', r'\\b뭐\\b', r'\\b어\\b',  r'\\b인제\\b', r'\\b이제\\b', r'\\b막\\b', r'\\b아\\b', r'\\b음\\b', r'\\b읍\\b', r'\\b오\\b', r'\\b으\\b',\n",
    "                      r'좋 ', r'\\b크\\b', r'\\b스\\b', r'\\. \\.', r'^\\s*\\.\\s{1}'] # r'name[0-9]\\S*'\n",
    "\n",
    "    # 커스텀 불용어 제거\n",
    "    for pattern in stopwords_pattern:\n",
    "        text = re.sub(pattern, '', text)\n",
    "    \n",
    "    # x를 포함한 단어 제거\n",
    "    text = re.sub(r'\\b[가-힣a-zA-Z]*[xX][가-힣a-zA-Z]*\\b', '', text)\n",
    "\n",
    "    # 단어가 두 번 이상 반복되는 경우 -> 1개로\n",
    "    # text = re.sub(r'\\b(\\w)\\s+\\1\\b', r'\\1', text)\n",
    "    # text = re.sub(r'\\b([가-힣a-zA-Z0-9_]+)\\s+\\1\\b', r'\\1', text)\n",
    "    text = re.sub(r'\\b(\\w+)\\b(?:\\s+\\1\\b)+', r'\\1', text)\n",
    "\n",
    "    # 공백 두 번 이상 연속 -> 1개로\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "\n",
    "    # 간단한 후처리\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the dataset\n",
    "def preprocess(path:str):\n",
    "    \n",
    "    # Load the dataset\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # remove_stopwords_exception\n",
    "    data = remove_stopwords_exception(data, path)\n",
    "\n",
    "    # correct_wrong_outputㅋ\n",
    "    data = correct_wrong_output(data, path)\n",
    "\n",
    "    # change_weird_output\n",
    "    data = change_weird_output(data, path)\n",
    "\n",
    "    # remove_sd_in_total_summary\n",
    "    data = remove_sd_in_total_summary(data, path)\n",
    "\n",
    "    # add_space_after_period and strip\n",
    "    data = add_space_after_period_and_remove_control_characters(data, path)\n",
    "    \n",
    "    # total_summary_generalization\n",
    "    data = total_summary_generalization(data, path)\n",
    "\n",
    "    # # remove_empty_utterance\n",
    "    # data = remove_empty_utterance(data)\n",
    "\n",
    "    # remove_duplicate_output_words\n",
    "    data = remove_duplicate_output_words(data, path)\n",
    "\n",
    "    # remove_hwaja_before_speaker_in_output\n",
    "    data = remove_hwaja_before_speaker_in_output(data, path)\n",
    "\n",
    "    # preprocess the dataset\n",
    "    for example in data:\n",
    "        for cvt in example['input']['conversation']:\n",
    "            cvt['utterance'] = remove_stopwords(cvt['utterance'])\n",
    "\n",
    "    # remove_empty_utterance\n",
    "    data = remove_empty_utterance(data)\n",
    "\n",
    "    # add_josa_after_speaker_in_output\n",
    "    data = add_josa_after_speaker_in_output(data, path)\n",
    "\n",
    "    # speaker_summary_generalization\n",
    "    data = speaker_summary_generalization(data, path)\n",
    "\n",
    "    # Save the preprocessed dataset\n",
    "    with open(path.split('/')[-1].split('_')[1].split('.')[0]+'.json', 'w') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Preprocessing of {path} is done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing of ../resource/data/일상대화요약_train.json is done!\n"
     ]
    }
   ],
   "source": [
    "preprocess('../resource/data/일상대화요약_train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing of ../resource/data/일상대화요약_dev.json is done!\n"
     ]
    }
   ],
   "source": [
    "preprocess('../resource/data/일상대화요약_dev.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing of ../resource/data/일상대화요약_test.json is done!\n"
     ]
    }
   ],
   "source": [
    "preprocess('../resource/data/일상대화요약_test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
