{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopword들을 찾고 제거하거나 1개로 통일하자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 양 옆에 공백을 두고 있지만 의미없는 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from copy import deepcopy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataframe(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read a json file and return a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    path (str): Path to the json file.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame of the json file.\n",
    "    \"\"\"\n",
    "    # Read the json file\n",
    "    with open(path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    # columns = ['id', 'conversation', 'subject_keyword', 'output']\n",
    "    df = pd.DataFrame(data)\n",
    "    df['conversation'] = df['input'].apply(lambda x: x['conversation'])\n",
    "    df['subject_keyword'] = df['input'].apply(lambda x: x['subject_keyword'])\n",
    "\n",
    "    # Drop the 'input' column\n",
    "    df.drop('input', axis=1, inplace=True)\n",
    "\n",
    "    # Speakers in the conversation\n",
    "    df['speakers'] = df['conversation'].apply(lambda turns: list(set(turn['speaker'] for turn in turns)))\n",
    "\n",
    "    # Reorder the columns\n",
    "    df = df[['id', 'conversation', 'subject_keyword', 'speakers', 'output']]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = make_dataframe('../resource/data/일상대화요약_train.json')\n",
    "# dev_df = make_dataframe('../resource/data/일상대화요약_dev.json')\n",
    "# test_df = make_dataframe('../resource/data/일상대화요약_test.json')\n",
    "train_df = make_dataframe('./train.json')\n",
    "dev_df = make_dataframe('./dev.json')\n",
    "test_df = make_dataframe('./test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the stopwords\n",
    "def find_stopwords(df: pd.DataFrame, pattern) -> set:\n",
    "    \"\"\"\n",
    "    Find the stopwords in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame of the json file.\n",
    "\n",
    "    Returns:\n",
    "    set: Set of stopwords.\n",
    "    \"\"\"\n",
    "    # Find the stopwords\n",
    "    stopwords = defaultdict(int)\n",
    "    stopwords_in_output = defaultdict(int)\n",
    "    utter_cnt = 0\n",
    "    output_cnt = 0\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        # Find the stopwords in the output\n",
    "        output = row['output']\n",
    "        stopwords_list = re.findall(pattern, output)\n",
    "        for stopword in stopwords_list:\n",
    "            stopwords_in_output[stopword] += 1\n",
    "            output_cnt+=1\n",
    "\n",
    "\n",
    "        # Find the stopwords in the conversation\n",
    "        for turn in row['conversation']:\n",
    "            utterance = turn['utterance']\n",
    "\n",
    "            # Find the stopwords and add them to stopwords\n",
    "            stopwords_list = re.findall(pattern, utterance)\n",
    "            for stopword in stopwords_list:\n",
    "                stopwords[stopword] += 1\n",
    "                utter_cnt+=1\n",
    "\n",
    "    # Make a Series of stopwords\n",
    "    stopwords = pd.Series(stopwords)\n",
    "    stopwords_in_output = pd.Series(stopwords_in_output)\n",
    "\n",
    "    print(f\"Number of stopwords in utterance : {len(stopwords)}\")\n",
    "    print(f\"Number of stopwords in output : {len(stopwords_in_output)}\")\n",
    "    print(f\"Frequency of stopwords in utterance : {utter_cnt}\")\n",
    "    print(f\"Frequency of stopwords in output : {output_cnt}\")\n",
    "    \n",
    "\n",
    "    return stopwords, stopwords_in_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stopwords in utterance : 694\n",
      "Number of stopwords in output : 223\n",
      "Frequency of stopwords in utterance : 40384\n",
      "Frequency of stopwords in output : 3801\n"
     ]
    }
   ],
   "source": [
    "stopwords, st_output = find_stopwords(train_df, pattern = r'(?:\\s+[가-힣]{1}\\s+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stopwords: 1\n",
      "Number of stopwords in the output: 0\n"
     ]
    }
   ],
   "source": [
    "stopwords, st_output = find_stopwords(test_df, pattern = r'\\b좀\\b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "좀    872\n",
       "dtype: int64"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2796"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.loc['좀']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st_output.loc['좀']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.sort_values(ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 거     5282\n",
       " 좀     2534\n",
       " 안     2465\n",
       " 또     2339\n",
       " 게     2235\n",
       " 한     1767\n",
       " 때     1438\n",
       " 잘     1286\n",
       " 다     1281\n",
       " 더     1055\n",
       " 수      869\n",
       " 못      782\n",
       " 할      768\n",
       " 해      712\n",
       " 걸      669\n",
       " 것      627\n",
       " 이      626\n",
       " 건      529\n",
       " 내      510\n",
       " 나      414\n",
       " 난      336\n",
       " 가      326\n",
       " 될      318\n",
       " 꼭      315\n",
       " 왜      308\n",
       " 참      304\n",
       " 갈      291\n",
       " 딱      285\n",
       " 제      276\n",
       " 데      270\n",
       " 몇      255\n",
       " 본      243\n",
       " 저      242\n",
       " 두      225\n",
       " 큰      223\n",
       " 볼      217\n",
       " 사      172\n",
       " 너      158\n",
       " 줄      154\n",
       " 된      153\n",
       " 집      143\n",
       " 살      141\n",
       " 뭘      137\n",
       " 날      134\n",
       " 원      130\n",
       " 네      129\n",
       " 적      128\n",
       " 세      117\n",
       " 돼      113\n",
       " 일      105\n",
       "dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found in '그래 그래서': 그래 그래서\n",
      "Match found in '아니 아니야': 아니 아니야\n",
      "Match found in '하지 하지만': 하지 하지만\n",
      "No match found in '먹먹 먹'\n",
      "Match found in '그래 그래': 그래 그래\n",
      "Match found in '응 응해': 응 응해\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 테스트 문자열들\n",
    "texts = [\n",
    "    \"그래 그래서\",   # 매칭됨\n",
    "    \"아니 아니야\",   # 매칭됨\n",
    "    \"하지 하지만\",   # 매칭됨\n",
    "    \"먹먹 먹\",       # 매칭되지 않음\n",
    "    \"그래 그래\",     # 매칭되지 않음\n",
    "    \"응 응해\",       # 매칭됨\n",
    "]\n",
    "\n",
    "# 정규 표현식 패턴\n",
    "pattern = r'\\b(\\w+)\\s+\\1(\\w*\\b)'\n",
    "\n",
    "# 각 문자열에 대해 패턴과 일치하는지 확인\n",
    "for text in texts:\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        print(f\"Match found in '{text}':\", match.group())\n",
    "    else:\n",
    "        print(f\"No match found in '{text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stopwords: 901\n",
      "Number of stopwords in the output: 5\n"
     ]
    }
   ],
   "source": [
    "sw, sw_output = find_stopwords(train_df, pattern = r'\\b\\s+(\\w+)\\s+\\1(\\w*)\\b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "두루        1\n",
       "할머니가      1\n",
       "깜짝        1\n",
       "좋은        1\n",
       "가도        1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "가    지고     41\n",
       "있    는       5\n",
       "내    가       5\n",
       "하    고       4\n",
       "많    이       4\n",
       "시국이  니만큼     3\n",
       "거    든       3\n",
       "제    대로      3\n",
       "하    는       3\n",
       "이    런       3\n",
       "그렇   게       3\n",
       "이    게       3\n",
       "나    도       3\n",
       "     중에      3\n",
       "물    론       2\n",
       "옷    을       2\n",
       "다    니고      2\n",
       "있    을       2\n",
       "나    는       2\n",
       "것    도       2\n",
       "거기   가       2\n",
       "그래   서       2\n",
       "다    녔던      2\n",
       "요    런       2\n",
       "가    장       2\n",
       "없    는       2\n",
       "거    기       2\n",
       "다    르고      2\n",
       "사    실       2\n",
       "학    교       2\n",
       "다    르게      2\n",
       "그거   를       2\n",
       "키우   는       2\n",
       "되    면       2\n",
       "우    리       2\n",
       "마    라톤      2\n",
       "너    도       2\n",
       "고    양이가     2\n",
       "먹    을       2\n",
       "있    어       2\n",
       "대    충       2\n",
       "일    단       2\n",
       "다    른데      2\n",
       "있    어서      2\n",
       "이    것도      2\n",
       "영    화를      2\n",
       "있    다       2\n",
       "언    제야      2\n",
       "요즘   은       2\n",
       "지금   도       2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw.sort_values(ascending=False, inplace=True)\n",
    "sw[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('있', '는')]\n",
      "[('내', '가')]\n",
      "[('하', '고')]\n",
      "[('많', '이')]\n",
      "[('하', '는')]\n",
      "[('그렇', '게')]\n",
      "[('나', '중에')]\n",
      "[('그래', '서'), ('거인이었', '거든요')]\n"
     ]
    }
   ],
   "source": [
    "samples = [\"그럼 네가 넷플릭스에서 봤었던 넷플릭스 영화만 있 있는 것도 있잖아.\", # 있 있는 : train 5,\n",
    "           \" 근데 우리 강아지는 내 내가 중학교 2학년 땐가 집을 잠깐 비운 적 있었는데\", # 내 내가 : train 5\n",
    "           \"안 먹이려고 하고 있 하 하고 있고요.\", # 하 하고: train 4\n",
    "           \"엄청 많 많이 본 거 같아.\", # 많 많이: train 4\n",
    "           \"영화여서 되게 좋아 하 하는 거 같아.\", # 하 하는: train 3\n",
    "           \"육전도 들어가고 그렇 그렇게 하면서\", # 그렇 그렇게: train 3\n",
    "           \"시즌 투 나왔을 때도 나 나중에 본다 나중에 아껴볼 거라고\", # 나 나중에: train 3\n",
    "           \"그래 그래서 애니메이션이 진격의 거인이었 거인이었거든요.\", # 거인이었 거인이: train 3\n",
    "           ]\n",
    "\n",
    "for sample in samples:\n",
    "    print(re.findall(r'\\b(\\w+)\\s+\\1(\\w*)\\b', sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'그래서 애니메이션이 진격의 거인이었거든요.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'\\b([가-힣]+)\\s+\\1([가-힣]*)\\b', r'\\1\\2','그래 그래서 애니메이션이 진격의 거인이었 거인이었거든요.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: '그래 그래서' => Modified: '그래서'\n",
      "Original: '아니 아니야' => Modified: '아니야'\n",
      "Original: '하지 하지만' => Modified: '하지만'\n",
      "Original: '먹먹 먹' => Modified: '먹먹 먹'\n",
      "Original: '그래 그래' => Modified: '그래 그래'\n",
      "Original: '응 응해' => Modified: '응해'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 테스트 문자열들\n",
    "texts = [\n",
    "    \"그래 그래서\",   # 매칭됨\n",
    "    \"아니 아니야\",   # 매칭됨\n",
    "    \"하지 하지만\",   # 매칭됨\n",
    "    \"먹먹 먹\",       # 매칭되지 않음\n",
    "    \"그래 그래\",     # 매칭되지 않음\n",
    "    \"응 응해\",       # 매칭됨\n",
    "]\n",
    "\n",
    "# 정규 표현식 패턴\n",
    "pattern = r'\\b([가-힣]+)\\s+\\1([가-힣]+)\\b'\n",
    "\n",
    "# 각 문자열에 대해 패턴과 일치하는지 확인하고 치환\n",
    "for text in texts:\n",
    "    result = re.sub(pattern, r'\\1\\2', text)\n",
    "    print(f\"Original: '{text}' => Modified: '{result}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X를 포함하는 단어들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stopwords: 59\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "xxx          4\n",
       "xx로          1\n",
       "x           28\n",
       "x는           2\n",
       "제x           1\n",
       "다니x          1\n",
       "즐겁더라고x       1\n",
       "xx케          1\n",
       "x케           1\n",
       "xx          15\n",
       "요즘x          1\n",
       "어디xx         1\n",
       "요x           1\n",
       "이x           3\n",
       "마마마마x        1\n",
       "근x           1\n",
       "싶x           1\n",
       "생각xx         1\n",
       "알았x          1\n",
       "같x           2\n",
       "운x을          1\n",
       "그x           2\n",
       "같으xx요        1\n",
       "그래x          1\n",
       "왜x면          1\n",
       "그xx          1\n",
       "x이지만         1\n",
       "그래갖고서x       1\n",
       "먹x           1\n",
       "그x고          1\n",
       "끊x고          1\n",
       "x다음에         1\n",
       "xx서          6\n",
       "한x           1\n",
       "x번           1\n",
       "xx으로         1\n",
       "올라x고         1\n",
       "x보적으로        1\n",
       "아이x          1\n",
       "xxxxx습니다     1\n",
       "x거           1\n",
       "xx도          1\n",
       "x짜           1\n",
       "무난했xx        1\n",
       "xxxx         2\n",
       "x데           4\n",
       "그x까          1\n",
       "같xx          1\n",
       "xx게          1\n",
       "x시           1\n",
       "해x고          1\n",
       "꼽으라x         1\n",
       "x니까          4\n",
       "x게           1\n",
       "xx까          1\n",
       "싶xx          1\n",
       "지난xx         1\n",
       "xx튼          1\n",
       "x까           1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_stopwords(test_df, pattern = r'\\b[가-힣a-zA-Z]*[xX][가-힣a-zA-Z]*\\b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stopwords: 59\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "xxx          4\n",
       "xx로          1\n",
       "x           28\n",
       "x는           2\n",
       "제x           1\n",
       "다니x          1\n",
       "즐겁더라고x       1\n",
       "xx케          1\n",
       "x케           1\n",
       "xx          15\n",
       "요즘x          1\n",
       "어디xx         1\n",
       "요x           1\n",
       "이x           3\n",
       "마마마마x        1\n",
       "근x           1\n",
       "싶x           1\n",
       "생각xx         1\n",
       "알았x          1\n",
       "같x           2\n",
       "운x을          1\n",
       "그x           2\n",
       "같으xx요        1\n",
       "그래x          1\n",
       "왜x면          1\n",
       "그xx          1\n",
       "x이지만         1\n",
       "그래갖고서x       1\n",
       "먹x           1\n",
       "그x고          1\n",
       "끊x고          1\n",
       "x다음에         1\n",
       "xx서          6\n",
       "한x           1\n",
       "x번           1\n",
       "xx으로         1\n",
       "올라x고         1\n",
       "x보적으로        1\n",
       "아이x          1\n",
       "xxxxx습니다     1\n",
       "x거           1\n",
       "xx도          1\n",
       "x짜           1\n",
       "무난했xx        1\n",
       "xxxx         2\n",
       "x데           4\n",
       "그x까          1\n",
       "같xx          1\n",
       "xx게          1\n",
       "x시           1\n",
       "해x고          1\n",
       "꼽으라x         1\n",
       "x니까          4\n",
       "x게           1\n",
       "xx까          1\n",
       "싶xx          1\n",
       "지난xx         1\n",
       "xx튼          1\n",
       "x까           1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_stopwords(test_df, pattern = r'\\b\\w*[xX]\\w*\\b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 제어문자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stopwords in utterance : 0\n",
      "Number of stopwords in output : 3\n",
      "Frequency of stopwords in utterance : 0\n",
      "Frequency of stopwords in output : 13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Series([], dtype: object),\n",
       " \b     5\n",
       " \n",
       "      6\n",
       " \\n    2\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_stopwords(train_df, pattern=r'[\\x00-\\x1F\\x7F]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 전처리 이후"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = make_dataframe('./train.json')\n",
    "dev_df = make_dataframe('./dev.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stopwords in utterance : 0\n",
      "Number of stopwords in output : 0\n",
      "Frequency of stopwords in utterance : 0\n",
      "Frequency of stopwords in output : 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Series([], dtype: object), Series([], dtype: object))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_stopwords(train_df, pattern=r'[\\x00-\\x1F\\x7F]')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
