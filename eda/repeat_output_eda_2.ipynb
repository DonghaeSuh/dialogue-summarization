{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 반복 단어, 문장 1개로 통일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge\n",
      "  Obtaining dependency information for rouge from https://files.pythonhosted.org/packages/32/7c/650ae86f92460e9e8ef969cc5008b24798dcf56a9a8947d04c78f550b3f5/rouge-1.0.1-py3-none-any.whl.metadata\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: six in c:\\users\\user\\anaconda3\\envs\\nlp1\\lib\\site-packages (from rouge) (1.16.0)\n",
      "Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: rouge\n",
      "Successfully installed rouge-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from copy import deepcopy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)\n",
    "import rouge\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataframe(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read a json file and return a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    path (str): Path to the json file.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame of the json file.\n",
    "    \"\"\"\n",
    "    # Read the json file\n",
    "    with open(path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    # columns = ['id', 'conversation', 'subject_keyword', 'output']\n",
    "    df = pd.DataFrame(data)\n",
    "    df['conversation'] = df['input'].apply(lambda x: x['conversation'])\n",
    "    df['subject_keyword'] = df['input'].apply(lambda x: x['subject_keyword'])\n",
    "\n",
    "    # Drop the 'input' column\n",
    "    df.drop('input', axis=1, inplace=True)\n",
    "\n",
    "    # Speakers in the conversation\n",
    "    df['speakers'] = df['conversation'].apply(lambda turns: list(set(turn['speaker'] for turn in turns)))\n",
    "\n",
    "    # Reorder the columns\n",
    "    df = df[['id', 'conversation', 'subject_keyword', 'speakers', 'output']]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = make_dataframe('../resource/data/일상대화요약_train.json')\n",
    "dev_df = make_dataframe('../resource/data/일상대화요약_dev.json')\n",
    "test_df = make_dataframe('../resource/data/일상대화요약_test.json')\n",
    "filtered_train_df = make_dataframe('./train.json')\n",
    "filtered_dev_df = make_dataframe('./dev.json')\n",
    "filtered_test_df = make_dataframe('./test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypernova = make_dataframe('../results/hypernova.json')\n",
    "cosmos = make_dataframe('../results/cosmos.json')\n",
    "cosmos2 = make_dataframe('../results/cosmos2.json')\n",
    "cosmos25 = make_dataframe('../results/cosmos_25.json')\n",
    "cosmosre12 = make_dataframe('../results/cosmos_re12.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## output 내의 반복되는 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output 내에 '\\b([가-힣a-zA-Z0-9_]+)\\s+\\1\\b'를 만족하는 부분이 있는지 확인하고 있다면 set에 추가하고 마지막에 return하는 함수\n",
    "\n",
    "def find_repeated_words(df: pd.DataFrame, pattern) -> set:\n",
    "    \"\"\"\n",
    "    Find repeated words in the 'output' column of the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame to search for repeated words.\n",
    "    pattern (str): Pattern to search for.\n",
    "\n",
    "    Returns:\n",
    "    set: Set of repeated words.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "\n",
    "    repeated_words = set()\n",
    "    for output in df['output']:\n",
    "        matches = re.findall(pattern, output)\n",
    "        repeated_words.update(matches)\n",
    "        count+=len(matches)\n",
    "\n",
    "    print(f\"Total number of repeated words found: {len(repeated_words)}\")\n",
    "    print(f\"Total number of repeated words found: {count}\")\n",
    "\n",
    "    return repeated_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of repeated words found: 8\n",
      "Total number of repeated words found: 12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'가도', '그리고', '깜짝', '대화에서', '두루', '좋은', '할머니가', '화자'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeated_words = find_repeated_words(filtered_train_df, r'\\b(\\w+)\\b(?:\\s+\\1\\b)+')\n",
    "repeated_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of repeated words found: 0\n",
      "Total number of repeated words found: 0\n"
     ]
    }
   ],
   "source": [
    "repeated_words = find_repeated_words(filtered_dev_df, r'\\b(\\w+)\\s+\\1\\b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of repeated words found: 0\n",
      "Total number of repeated words found: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeated_words = find_repeated_words(cosmos25, r'\\b(\\w+)\\s+\\1\\b')\n",
    "repeated_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of repeated words found: 2\n",
      "Total number of repeated words found: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'깜짝', '두루'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 전처리 이후\n",
    "filtered_train_df = make_dataframe('./train.json')\n",
    "filtered_dev_df = make_dataframe('./dev.json')\n",
    "filtered_test_df = make_dataframe('./test.json')\n",
    "\n",
    "repeated_words = find_repeated_words(filtered_train_df, r'\\b(\\w+)\\b(?:\\s+\\1\\b)+')\n",
    "repeated_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## output 내의 반복되는 문장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output 내의 반복되는 문장 찾기\n",
    "rouge_score = rouge.Rouge().get_scores\n",
    "\n",
    "def find_repeated_sentence(df: pd.DataFrame) -> set:\n",
    "    \"\"\"\n",
    "    Find repeated sentences in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame of the output.\n",
    "\n",
    "    Returns:\n",
    "    set: Set of repeated sentences.\n",
    "    \"\"\"\n",
    "    exact_repeated_sentences = set()\n",
    "    high_similarity_repeated_sentences = defaultdict(list)\n",
    "    exact_cnt = 0\n",
    "    high_similarity_cnt = 0\n",
    "    exact_idxes = set()\n",
    "    high_similarity_idxes = set()\n",
    "\n",
    "\n",
    "    for idx, output in tqdm(enumerate(df['output']), total=len(df)):\n",
    "        output_sentences = output.split('.')[:-1]\n",
    "\n",
    "        # exact_repeated_sentences\n",
    "        for sentence in output_sentences: # 마지막은 빈 문자열이므로 제외\n",
    "            if output.count(sentence) > 1:\n",
    "                exact_repeated_sentences.add(sentence)\n",
    "                output_sentences.remove(sentence)\n",
    "                exact_cnt+=1\n",
    "                exact_idxes.add(idx)\n",
    "\n",
    "        # high_similarity_repeated_sentences\n",
    "        for sent_idx in range(1,len(output_sentences)-1):\n",
    "            for unique_sentence in output_sentences[:sent_idx]:\n",
    "                scores = rouge_score(output_sentences[sent_idx], unique_sentence)\n",
    "                if scores[0]['rouge-1']['f'] > 0.6:\n",
    "                    high_similarity_repeated_sentences[unique_sentence].append((idx, unique_sentence, output_sentences[sent_idx]))\n",
    "                    high_similarity_cnt+=1\n",
    "                    high_similarity_idxes.add(idx)\n",
    "\n",
    "    print(f\"Total number of exact repeated sentences found: {exact_cnt}\")\n",
    "    print(f\"the number of exact repeated sentences found: {len(exact_repeated_sentences)}\")\n",
    "    print(f\"Total number of high similarity repeated sentences found: {high_similarity_cnt}\")\n",
    "    print(f\"the number of high similarity repeated sentences found: {len(high_similarity_repeated_sentences)}\")\n",
    "    print(f\"Exact repeated sentences found in the following indexes: {exact_idxes}\")\n",
    "    print(f\"High similarity repeated sentences found in the following indexes: {high_similarity_idxes}\")\n",
    "\n",
    "    return exact_repeated_sentences, high_similarity_repeated_sentences, exact_idxes, high_similarity_idxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of exact repeated sentences found: 0\n",
      "the number of exact repeated sentences found: 0\n",
      "Total number of high similarity repeated sentences found: 0\n",
      "the number of high similarity repeated sentences found: 0\n",
      "Exact repeated sentences found in the following indexes: set()\n",
      "High similarity repeated sentences found in the following indexes: set()\n"
     ]
    }
   ],
   "source": [
    "repeated_sentences, hs, _, _ = find_repeated_sentence(filtered_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of exact repeated sentences found: 1\n",
      "the number of exact repeated sentences found: 1\n",
      "Total number of high similarity repeated sentences found: 1\n",
      "the number of high similarity repeated sentences found: 1\n",
      "Exact repeated sentences found in the following indexes: [92]\n",
      "High similarity repeated sentences found in the following indexes: [92]\n"
     ]
    }
   ],
   "source": [
    "repeated_sentences = find_repeated_sentence(filtered_dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'SD2000862는 엔시티의 보스 착장 중 제복 착장이 멤버들의 피지컬 덕분에 좋았고 터프하게 랩을 하는 마크의 모습이 인상깊었다고 말했습니다'},\n",
       " {('SD2000862는 엔시티의 보스 착장 중 제복 착장이 멤버들의 피지컬 덕분에 좋았고 터프하게 랩을 하는 마크의 모습이 인상깊었다고 말했습니다',\n",
       "   ' SD2000862는 엔시티의 보스 착장 중 제복 착장이 멤버들의 피지컬 덕분에 좋았고 터프하게 랩을 하는 마크의 모습이 인상깊었다고 말했습니다')})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeated_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 중복 제거 이후"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of exact repeated sentences found: 0\n",
      "the number of exact repeated sentences found: 0\n",
      "Total number of high similarity repeated sentences found: 0\n",
      "the number of high similarity repeated sentences found: 0\n",
      "Exact repeated sentences found in the following indexes: set()\n",
      "High similarity repeated sentences found in the following indexes: set()\n"
     ]
    }
   ],
   "source": [
    "# 전처리 이후\n",
    "filtered_train_df = make_dataframe('./train.json')\n",
    "filtered_dev_df = make_dataframe('./dev.json')\n",
    "filtered_test_df = make_dataframe('./test.json')\n",
    "repeated_sentences, hs = find_repeated_sentence(filtered_dev_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### result 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/408 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 408/408 [00:00<00:00, 855.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of exact repeated sentences found: 22\n",
      "the number of exact repeated sentences found: 9\n",
      "Total number of high similarity repeated sentences found: 70\n",
      "the number of high similarity repeated sentences found: 35\n",
      "Exact repeated sentences found in the following indexes: {166, 328, 269, 23, 216, 219, 221}\n",
      "High similarity repeated sentences found in the following indexes: {129, 130, 261, 137, 396, 14, 15, 149, 26, 282, 166, 169, 302, 310, 316, 68, 328, 204, 206, 336, 219, 349, 98, 356, 103, 360, 105, 111, 369, 125}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "repeated_sentences, high_similarity_repeated_sentences, exact_idxes, high_idxes = find_repeated_sentence(cosmos25) # cosmosre12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_idxes in high_idxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' SD2000282는 SD2000283이 많이 먹는 것 같으니 차슈 덩밥을 먹고 싶어서 상무지구의 차슈 덩밥을 먹으러 가자고 말했습니다',\n",
       " ' SD2000282는 SD2000283이 많이 먹는 것 같으니 차슈 덮밥을 먹고 싶어서 상무지구의 차슈 덮밥을 먹으러 가자고 말했습니다',\n",
       " ' SD2000287은 중학교 1학년 때 동아리로 연극동아리에 가입하게 된 이유는 선생님께서 추천해주셨기 때문이라고 답했습니다',\n",
       " ' 단점으로는 감기와 추위가 심해 밖에서의 활동이 제한될 수 있다고 했습니다',\n",
       " ' 또 대전여중 앞쪽에 두부 두루치기가 있고, 성모병원 앞에도 두부 두루치기가 있다고 말했습니다',\n",
       " ' 또 자신이 선물을 선택할 때는 가성비보다는 자신이 좋아하는 것을 선물한다고 했습니다',\n",
       " ' 또 초등학교 때 가장 친했던 친구가 있었는지 물어봤으며, 중학교 때 연극동아리를 하면서 느낀 장점과 단점을 질문했습니다',\n",
       " ' 또한 혈액형은 성격과 관련이 없다는 것을 알고 있어서 혈액형에 의존하여 성격을 판단하지 않는다고 말했습니다',\n",
       " ' 여행 방식은 즉흥적인 여행을 선호한다고 말했습니다'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeated_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14][중복되는 문장중에서 가장 첫 번째로 등장하는 문장] :  SD2000058은 가족 중에서 가장 건강에 염려스러운 것은 치매라고 말했습니다 \n",
      "\n",
      " -   또 가족 중에서 가장 건강에 염려스러운 것은 치매라고 말했고, 가족 중에서 가장 건강에 염려스러운 것은 치매라고 말했습니다\n",
      " -   SD2000059는 가족 중에서 가장 건강에 염려스러운 것은 고혈압이라고 말했습니다\n",
      "\n",
      "\n",
      "\n",
      "[14][중복되는 문장중에서 가장 첫 번째로 등장하는 문장] :  또 가족 중에서 가장 건강에 염려스러운 것은 치매라고 말했고, 가족 중에서 가장 건강에 염려스러운 것은 치매라고 말했습니다 \n",
      "\n",
      " -   SD2000059는 가족 중에서 가장 건강에 염려스러운 것은 고혈압이라고 말했습니다\n",
      "\n",
      "\n",
      "\n",
      "[15][중복되는 문장중에서 가장 첫 번째로 등장하는 문장] :  SD2000059는 코로나에 걸렸을 때 극심한 근육통, 고열, 기침 등의 증상을 보일 수도 있으며 무증상 환자도 있을 수 있다고 말했습니다 \n",
      "\n",
      " -   또 코로나에 걸리면 근육통, 고열, 기침 등의 증상을 보일 수 있으며 무증상 환자도 있을 수 있다고 말했습니다\n",
      "\n",
      "\n",
      "\n",
      "[26][중복되는 문장중에서 가장 첫 번째로 등장하는 문장] :  SD2000066은 남자 친구가 생긴다면 가장 먹고 싶은 음식은 이탈리아 음식이라고 말했습니다 \n",
      "\n",
      " -   SD2000067은 남자 친구가 생긴다면 가장 먹고 싶은 음식은 이탈리아 음식이라고 말했습니다\n",
      "\n",
      "\n",
      "\n",
      "[68][중복되는 문장중에서 가장 첫 번째로 등장하는 문장] :  그리고 가족이 타인으로 대우받는다고 생각하며 가족과의 갈등을 해결하기 위해 타인을 기준으로 생각해보려고 한다며 가족과의 갈등을 해결하려는 방법을 설명했습니다 \n",
      "\n",
      " -   SD2000114는 가족이 타인으로 대우받는다고 생각하며 가족과의 갈등을 해결하기 위해 가족과의 갈등을 해결하려는 방법을 설명했습니다\n",
      "\n",
      "\n",
      "\n",
      "[98][중복되는 문장중에서 가장 첫 번째로 등장하는 문장] :  또 중학교 때는 조퇴해 집에서 땡까땡까 놀았고, 고등학교 때는 친구들과 함께 과자 먹으며 이야기하는 것을 좋아했다고 했습니다 \n",
      "\n",
      " -   또 중학교 때는 조퇴해 땡까땡까 놀았고, 고등학교 때는 친구들과 함께 과자 먹으며 이야기하는 것을 좋아했다고 했습니다\n",
      "\n",
      "\n",
      "\n",
      "[98][중복되는 문장중에서 가장 첫 번째로 등장하는 문장] :  SD2000148은 고등학교 때는 친구들과 싸울 일이 많았고, 친구들이 착하지만 수행평가를 할 때 어려워서 힘들었다고 했습니다 \n",
      "\n",
      " -   또 고등학교 때는 친구들과 싸울 일이 많았고, 친구들이 착하지만 수행평가를 할 때 어려워서 힘들었다고 했습니다\n",
      "\n",
      "\n",
      "\n",
      "[103][중복되는 문장중에서 가장 첫 번째로 등장하는 문장] :  가족과 함께 가는다면 단양을 추천하며, 친구들과 함께 가는다면 어디든 좋을 것 같다고 말했습니다 \n",
      "\n",
      " -   가족과 함께 가는다면 어디든 좋을 것 같고, 친구들과 함께 가는다면 어디든 좋을 것 같다고 말했습니다\n",
      "\n",
      "\n",
      "\n",
      "[105][중복되는 문장중에서 가장 첫 번째로 등장하는 문장] :  또한 영화 배우로는 김혜수, 손예진, 이병헌을 좋아한다고 말했습니다 \n",
      "\n",
      " -   또한 영화 배우로는 이병헌을 좋아한다고 말했습니다\n",
      "\n",
      "\n",
      "\n",
      "[111][중복되는 문장중에서 가장 첫 번째로 등장하는 문장] :  SD2000160은 해외 여행지 추천과 국내 여행지 추천을 요청했고, 음식 맛 좋은 곳 추천을 요청했습니다 \n",
      "\n",
      " -   SD2000161은 해외 여행지 추천과 국내 여행지 추천, 음식 맛 좋은 곳 추천, 관광지 추천을 했습니다\n",
      " -   SD2000160은 해외 여행지 추천을 요청했고, 국내 여행지 추천을 요청했습니다\n",
      "\n",
      "\n",
      "\n",
      "[125][중복되는 문장중에서 가장 첫 번째로 등장하는 문장] :  SD2002376은 상대방에게 최근에 연애를 했다고 말하며 상대방보다 연애를 빨리 식는 편이고, 상대방보다 연애를 더 오래하는 편이라고 말했습니다 \n",
      "\n",
      " -   SD2002375는 상대방에게 최근에 연애를 했다고 말하며 상대방보다 연애를 오래하는 편이며, 상대방보다 연애를 더 자주 하는 편이라고 말했습니다\n",
      "\n",
      "\n",
      "\n",
      "[129][중복되는 문장중에서 가장 첫 번째로 등장하는 문장] :  SD2000174는 최근에 본 영화로는 1927를 꼽았으며, 영화관에 혼자 갔을 때 영화가 생각보다 재밌어서 감동받았다고 말했습니다 \n",
      "\n",
      " -   SD2000175는 최근에 본 영화로는 1917를 꼽았으며, 영화관에 혼자 갔을 때 영화가 생각보다 재밌어서 감동받았다고 말했습니다\n",
      "\n",
      "\n",
      "\n",
      "[130][중복되는 문장중에서 가장 첫 번째로 등장하는 문장] :  하지만 기생충은 쉽게 보급형으로 만들어져 재밌게 봤다며, 영화를 보는데 무슨 느낌인지 구체적으로 말할 수 없어서 실루엣만 알겠는 영화는 별로 좋아하지 않는다고 했습니다 \n",
      "\n",
      " -   SD2000174는 영화를 보는데 어떤 느낌인지 구체적으로 말할 수 없어서 실루엣만 알겠는 영화는 별로 좋아하지 않는다고 했습니다\n",
      "\n",
      "\n",
      "\n",
      "[130][중복되는 문장중에서 가장 첫 번째로 등장하는 문장] :  또한 이창동 감독의 영화는 어려워도 재미가 있고 의미가 있는 영화들이 많아서 인기가 많았을 것 같다고 했습니다 \n",
      "\n",
      " -   SD2000175는 이창동 감독의 영화는 어려워도 재미가 있고 의미가 있는 영화들이 많아서 인기가 많았을 것 같다고 말했습니다\n",
      "\n",
      "\n",
      "\n",
      "[137][중복되는 문장중에서 가장 첫 번째로 등장하는 문장] :  또 돈에 구애받지 않고 갈 수 있는 곳을 꼽으면 스위스를 가장 가고 싶다고 말했고, 유럽의 중립적인 나라로 평화로운 풍경이 인상적이라고 설명했습니다 \n",
      "\n",
      " -   또 돈에 구애받지 않고 갈 수 있는 곳을 꼽으면 스페인을 가장 가고 싶다고 말했고, 독일은 계획적인 도시여서 가보고 싶다고 했습니다\n",
      "\n",
      "\n",
      "\n",
      "[149][중복되는 문장중에서 가장 첫 번째로 등장하는 문장] : 두 화자는 이 대화에서 상대방에게 자신이 가장 좋아하는 남자의 유형에 대해 이야기했습니다 \n",
      "\n",
      " -   SD2002377은 자신의 첫사랑 이야기를 말하면서 상대방에게 자신이 가장 좋아하는 남자의 유형에 대해 이야기했습니다\n",
      "\n",
      "\n",
      "\n",
      "[166][중복되는 문장중에서 가장 첫 번째로 등장하는 문장] :  광천식당은 20대 때부터 핫플레이스로 유명했고, 두부 두루치기, 오징어 두루치기, 칼국수를 제공합니다 \n",
      "\n",
      " -   SD2000204는 대전은 두부 두루치기, 칼국수가 유명하며, 광천식당은 20대 때부터 핫플레이스로 유명했고, 두부 두루치기, 오징어 두루치기, 칼국수를 제공한다고 말했습니다\n",
      "\n",
      "\n",
      "\n",
      "[169][중복되는 문장중에서 가장 첫 번째로 등장하는 문장] :  SD2002371은 남편과 연애했던 상대와 결혼한 남편의 차이를 설명했습니다 \n",
      "\n",
      " -   SD2002372는 남편과 연애할 때와 결혼한 남편의 차이를 설명했습니다\n",
      "\n",
      "\n",
      "\n",
      "[204][중복되는 문장중에서 가장 첫 번째로 등장하는 문장] :  SD2000261은 초등학교 6학년 때의 담임 선생님과 고등학교 1학년 때의 담임 선생님을 기억에 남는 선생님이라고 말했습니다 \n",
      "\n",
      " -   SD2000263은 고등학교 1학년 때의 담임 선생님을 기억에 남는 선생님이라고 말했습니다\n",
      "\n",
      "\n",
      "\n",
      "[206][중복되는 문장중에서 가장 첫 번째로 등장하는 문장] :  또 인스턴트 식품을 줄이기 위해 노력하고 있으며, 아이들에게 인스턴트 식품을 줄이기 위해 노력하고 있다고 말했습니다 \n",
      "\n",
      " -   또 인스턴트 식품을 줄이기 위해 무첨가 제품을 선택하려고 노력하고 있다고 말했습니다\n",
      "\n",
      "\n",
      "\n",
      "[219][중복되는 문장중에서 가장 첫 번째로 등장하는 문장] :  SD2000283은 SD2000282가 맨날 먹는 니뽕, 내뽕, 짬뽕을 먹으려 한다며 차슈 덮밥을 먹고 싶다고 말했습니다 \n",
      "\n",
      " -   SD2000283은 SD2000282가 맨날 먹는 짬뽕을 먹으려 한다며 차슈 덮밥을 먹고 싶다고 말했습니다\n",
      " -   SD2000283은 SD2000282가 맨날 먹는 짬뽕을 먹으려 한다며 차슈 덮밥을 먹고 싶다고 말했습니다\n",
      " -   SD2000283은 SD2000282가 맨날 먹는 짬뽕을 먹으려 한다며 차슈 덩밥을 먹고 싶다고 말했습니다\n",
      " -   SD2000283은 SD2000282가 맨날 먹는 짬뽕을 먹으려 한다며 차슈 덩밥을 먹고 싶다고 말했습니다\n",
      " -   SD2000283은 SD2000282가 맨날 먹는 짬뽕을 먹으려 한다며 차슈 덩밥을 먹고 싶다고 말했습니다\n",
      " -   SD2000283은 SD2000282가 맨날 먹는 짬뽕을 먹으려 한다며 차슈 덩밥을 먹고 싶다고 말했습니다\n",
      " -   SD2000283은 SD2000282가 맨날 먹는 짬뽕을 먹으려 한다며 차슈 덩밥을 먹고 싶다고 말했습니다\n",
      " -   SD2000283은 SD2000282가 맨날 먹는 짬뽕을 먹으려 한다며 차슈 덩밥을 먹고 싶다고 말했습니다\n",
      "\n",
      "\n",
      "\n",
      "[219][중복되는 문장중에서 가장 첫 번째로 등장하는 문장] :  SD2000283은 SD2000282가 맨날 먹는 짬뽕을 먹으려 한다며 차슈 덮밥을 먹고 싶다고 말했습니다 \n",
      "\n",
      " -   SD2000283은 SD2000282가 맨날 먹는 짬뽕을 먹으려 한다며 차슈 덮밥을 먹고 싶다고 말했습니다\n",
      " -   SD2000283은 SD2000282가 맨날 먹는 짬뽕을 먹으려 한다며 차슈 덩밥을 먹고 싶다고 말했습니다\n",
      " -   SD2000283은 SD2000282가 맨날 먹는 짬뽕을 먹으려 한다며 차슈 덩밥을 먹고 싶다고 말했습니다\n",
      " -   SD2000283은 SD2000282가 맨날 먹는 짬뽕을 먹으려 한다며 차슈 덩밥을 먹고 싶다고 말했습니다\n",
      " -   SD2000283은 SD2000282가 맨날 먹는 짬뽕을 먹으려 한다며 차슈 덩밥을 먹고 싶다고 말했습니다\n",
      " -   SD2000283은 SD2000282가 맨날 먹는 짬뽕을 먹으려 한다며 차슈 덩밥을 먹고 싶다고 말했습니다\n",
      " -   SD2000283은 SD2000282가 맨날 먹는 짬뽕을 먹으려 한다며 차슈 덩밥을 먹고 싶다고 말했습니다\n",
      " -   SD2000283은 SD2000282가 맨날 먹는 짬뽕을 먹으려 한다며 차슈 덩밥을 먹고 싶다고 말했습니다\n",
      " -   SD2000283은 SD2000282가 맨날 먹는 짬뽕을 먹으려 한다며 차슈 덩밥을 먹고 싶다고 말했습니다\n",
      " -   SD2000283은 SD2000282가 맨날 먹는 짬뽕을 먹으려 한다며 차슈 덩밥을 먹고 싶다고 말했습니다\n",
      " -   SD2000283은 SD2000282가 맨날 먹는 짬뽕을 먹으려 한다며 차슈 덩밥을 먹고 싶다고 말했습니다\n",
      " -   SD2000283은 SD2000282가 맨날 먹는 짬뽕을 먹으려 한다며 차슈 덩밥을 먹고 싶다고 말했습니다\n",
      " -   SD2000283은 SD2000282가 맨날 먹는 짬뽕을 먹으려 한다며 차슈 덩밥을 먹고 싶다고 말했습니다\n",
      "\n",
      "\n",
      "\n",
      "[219][중복되는 문장중에서 가장 첫 번째로 등장하는 문장] :  SD2000283은 SD2000282가 맨날 먹는 짬뽕을 먹으려 한다며 차슈 덩밥을 먹고 싶다고 말했습니다 \n",
      "\n",
      " -   SD2000283은 SD2000282가 맨날 먹는 짬뽕을 먹으려 한다며 차슈 덩밥을 먹고 싶다고 말했습니다\n",
      " -   SD2000283은 SD2000282가 맨날 먹는 짬뽕을 먹으려 한다며 차슈 덩밥을 먹고 싶다고 말했습니다\n",
      " -   SD2000283은 SD2000282가 맨날 먹는 짬뽕을 먹으려 한다며 차슈 덩밥을 먹고 싶다고 말했습니다\n",
      " -   SD2000283은 SD2000282가 맨날 먹는 짬뽕을 먹으려 한다며 차슈 덩밥을 먹고 싶다고 말했습니다\n",
      " -   SD2000283은 SD2000282가 맨날 먹는 짬뽕을 먹으려 한다며 차슈 덩밥을 먹고 싶다고 말했습니다\n",
      " -   SD2000283은 SD2000282가 맨날 먹는 짬뽕을 먹으려 한다며 차슈 덩밥을 먹고 싶다고 말했습니다\n",
      " -   SD2000283은 SD2000282가 맨날 먹는 짬뽕을 먹으려 한다며 차슈 덩밥을 먹고 싶다고 말했습니다\n",
      " -   SD2000283은 SD2000282가 맨날 먹는 짬뽕을 먹으려 한다며 차슈 덩밥을 먹고 싶다고 말했습니다\n",
      " -   SD2000283은 SD2000282가 맨날 먹는 짬뽕을 먹으려 한다며 차슈 덩밥을 먹고 싶다고 말했습니다\n",
      " -   SD2000283은 SD2000282가 맨날 먹는 짬뽕을 먹으려 한다며 차슈 덩밥을 먹고 싶다고 말했습니다\n",
      " -   SD2000283은 SD2000282가 맨날 먹는 짬뽕을 먹으려 한다며 차슈 덩밥을 먹고 싶다고 말했습니다\n",
      " -   SD2000283은 SD2000282가 맨날 먹는 짬뽕을 먹으려 한다며 차슈 덩밥을 먹고 싶다고 말했습니다\n",
      " -   SD2000283은 SD2000282가 맨날 먹는 짬뽕을 먹으려 한다며 차슈 덩밥을 먹고 싶다고 말했습니다\n",
      " -   SD2000283은 SD2000282가 맨날 먹는 짬뽕을 먹으려 한다며 차슈 덩밥을 먹고 싶다고 말했습니다\n",
      " -   SD2000283은 SD2000282가 맨날 먹는 짬뽕을 먹으려 한다며 차슈 덩밥을 먹고 싶다고 말했습니다\n",
      "\n",
      "\n",
      "\n",
      "[261][중복되는 문장중에서 가장 첫 번째로 등장하는 문장] :  SD2000351은 친한 친구에게는 4~5만원, 친하지 않은 친구에게는 2만원 이하의 선물을 주는 편이라고 말했습니다 \n",
      "\n",
      " -   SD2000350은 친밀도가 높은 친구에게는 4~5만원, 친하지 않은 친구에게는 2만원 이하의 선물을 주는 편이라고 말했습니다\n",
      "\n",
      "\n",
      "\n",
      "[282][중복되는 문장중에서 가장 첫 번째로 등장하는 문장] :  SD2001639는 임대차 보호법이 5년이라는 기간 때문에 임대인에게 많은 권리가 주어져서 임대차 종료 후 임대인에게 큰 손실을 주는 경우가 많다고 이야기했습니다 \n",
      "\n",
      " -   SD2001640은 임대차 보호법이 5년이라는 기간 때문에 임대인에게 많은 권리가 주어져서 임대인에게 큰 손실을 주는 경우가 많다고 이야기했습니다\n",
      "\n",
      "\n",
      "\n",
      "[302][중복되는 문장중에서 가장 첫 번째로 등장하는 문장] :  SD2000481은 최근에 옻닭을 먹어본 적이 있다고 말하며 옻닭을 좋아한다고 했습니다 \n",
      "\n",
      " -   SD2000482는 최근에 백숙을 먹어본 적이 있다고 말하며 백숙을 좋아한다고 했습니다\n",
      "\n",
      "\n",
      "\n",
      "[310][중복되는 문장중에서 가장 첫 번째로 등장하는 문장] :  또 자신은 문구사 아르바이트를 추천하고 싶다고 말했습니다 \n",
      "\n",
      " -   또 카페 아르바이트를 도전해보고 싶다고 말했습니다\n",
      "\n",
      "\n",
      "\n",
      "[316][중복되는 문장중에서 가장 첫 번째로 등장하는 문장] :  또한 지인에게 직접 케이크를 만들어 주었을 때 기분이 좋았다고 말하며, 선물을 주는 것이 자신에게도 기분이 좋다고 말했습니다 \n",
      "\n",
      " -   또한 지인에게 꽃을 선물했을 때 기분이 좋았다고 말하며, 선물 받는 것보다 선물을 주는 것이 자신에게도 기분이 좋다고 말했습니다\n",
      "\n",
      "\n",
      "\n",
      "[328][중복되는 문장중에서 가장 첫 번째로 등장하는 문장] :  SD2002384는 엠비티아이가 엔프피(EF)인지 엔프제(IJ)인지에 따라 성격이 달라진다고 말했습니다 \n",
      "\n",
      " -   SD2002385는 엠비티아이가 EF인지 IJ인지에 따라 성격이 달라진다고 말했습니다\n",
      "\n",
      "\n",
      "\n",
      "[336][중복되는 문장중에서 가장 첫 번째로 등장하는 문장] :  SD2002390은 간호사, 유아교육과, 군인, 소방관, 호텔리어, 빵 가게 등 6개의 꿈이 있다고 말했습니다 \n",
      "\n",
      " -   SD2002391은 유아교육과, 경찰, 군인, 소방관, 호텔리어, 빵 가게 등 6개의 꿈이 있다고 말했습니다\n",
      "\n",
      "\n",
      "\n",
      "[349][중복되는 문장중에서 가장 첫 번째로 등장하는 문장] :  SD2000544는 어린 시절 가족여행 중 가장 기억에 남는 것은 돌머리 해수욕장에서 장어를 잡았던 것이며, 가족이 함께 텐트를 치면서 함께하는 시간이 좋았다고 말했습니다 \n",
      "\n",
      " -   SD2000545는 가족여행 중 가장 기억에 남는 것은 아버지와 함께 캠프를 떠난 것이며, 가족이 함께 텐트를 치면서 함께하는 시간이 좋았다고 말했습니다\n",
      "\n",
      "\n",
      "\n",
      "[356][중복되는 문장중에서 가장 첫 번째로 등장하는 문장] :  특히 평일에는 학교를 가지 않아 혼자 남아서 배달 음식을 많이 시켜 먹으며, 가장 많이 시키는 것은 떡볶이와 치킨이라고 했습니다 \n",
      "\n",
      " -   SD2000552는 평일에는 학교를 가지 않아 배달 음식을 많이 시켜 먹으며, 가장 많이 시키는 것은 애플 음식이라고 말했습니다\n",
      "\n",
      "\n",
      "\n",
      "[360][중복되는 문장중에서 가장 첫 번째로 등장하는 문장] :  또한, 반려동물에 대한 사회적 인식이 개선되었으며, 반려동물을 키우는 사람들이 많아졌다고 말했습니다 \n",
      "\n",
      " -   SD2000563은 반려동물에 대한 사회적 인식이 개선되었다고 말하며, 반려동물을 키우는 사람들에게 감탄한다고 말했습니다\n",
      "\n",
      "\n",
      "\n",
      "[369][중복되는 문장중에서 가장 첫 번째로 등장하는 문장] :  연애관은 데이트는 꼭 해야 하는 것은 아니며, 상대방이 시키는 것을 억지로 하는 것은 싫다고 했습니다 \n",
      "\n",
      " -   또한 데이트는 꼭 해야 하는 것은 아니며, 상대방이 시키는 것을 억지로 하는 것은 싫다고 했습니다\n",
      "\n",
      "\n",
      "\n",
      "[396][중복되는 문장중에서 가장 첫 번째로 등장하는 문장] :  SD2000666은 건강을 위해 보조 식품을 먹고 있다고 말했습니다 \n",
      "\n",
      " -   또한 눈 건강을 위해 특별히 챙기는 식품을 먹고 있다고 말했습니다\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in high_similarity_repeated_sentences.keys():\n",
    "    idx = high_similarity_repeated_sentences[key][0][0]\n",
    "    print(f\"[{idx}][중복되는 문장중에서 가장 첫 번째로 등장하는 문장] :\", key, '\\n')\n",
    "    for sentence in high_similarity_repeated_sentences[key]:\n",
    "        print(' - ', sentence[2])\n",
    "    print('\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_outputs(idx: int):\n",
    "    \"\"\"\n",
    "    Compare the outputs of the different models.\n",
    "\n",
    "    Parameters:\n",
    "    idx (int): Index of the output to compare.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    print(f\"Index: {idx}\")\n",
    "    print(f\"Hypernova: {hypernova['output'][idx]}\")\n",
    "    print(f\"Cosmos2: {cosmos2['output'][idx]}\")\n",
    "    print(f\"Cosmos25: {cosmos25['output'][idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 365\n",
      "Hypernova: 이 대화에서 SD2000568과 SD2000569는 국내외 여행지에 대해 이야기했습니다. SD2000568은 국내 여행지로는 서울이나 자연경관을 볼 수 있는 한적한 곳을 가보고 싶다고 말했습니다. 해외 여행지로는 일본, 제주도, 남도, 남해, 전라도, 터키를 가보고 싶다고 했습니다. 가족과 휴양지로는 일본을 추천했습니다. 해외 여행지는 남아공, 멕시코, 브라질, 중국을 가보고 싶지 않다고 했습니다. SD2000569은 부산의 해운대, 동백섬, 광안리, 송도 케이블 카, 송도 조개구이, 캠핑을 추천했습니다. 가족 휴양지는 몰디브, 발리, 인도, 남아공, 인도네시아를 추천했습니다. 또 해외 여행지로는 유럽, 프랑스 파리를 추천했습니다.\n",
      "Cosmos2: 두 화자는 이 대화에서 가보고 싶은 여행지, 가보고 싶은 해외 여행지, 가보고 싶은 휴양지 등에 대해 이야기했습니다. SD2000568은 국내 여행지로는 서울이나 자연경관을 볼 수 있는 한적한 곳을 추천했고, 해외 여행지로는 일본, 터키, 남도, 남해, 전라도, 제주도 등을 추천했습니다. 가족과 함께 가보고 싶은 휴양지는 일본이고, 가보고 싶은 해외 여행지는 터키라고 했습니다. 또한 가보고 싶은 휴양지로는 일본, 남도, 남해, 제주도, 전라도를 추천했으며, 가보고 싶은 해외 여행지는 터키라고 했습니다. SD2000569는 국내 여행지로는 해운대, 광안리, 송도 케이블카, 송도 조개 구, 캠핑을 추천했고, 가족과 함께 가보고 싶은 휴양지는 몰디브, 발리라고 했습니다. 또한 가보고 싶은 해외 여행지는 프랑스 파리라고 했습니다.\n",
      "Cosmos25: 두 화자는 이 대화에서 가보고 싶은 여행지, 가보고 싶은 해외 여행지, 가보고 싶은 휴양지 등에 대해 이야기했습니다. SD2000568은 국내 여행지로는 서울이나 자연경관을 볼 수 있는 곳을 추천했고, 해외 여행지로는 일본, 터키, 일본을 추천했습니다. 가족과 함께 가보고 싶은 휴양지는 일본이고, 가보고 싶은 해외 여행지는 남도, 남해, 제주도, 전라도, 일본, 터키, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공\n"
     ]
    }
   ],
   "source": [
    "compare_outputs(365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' SD2000282는 SD2000283이 많이 먹는 것 같으니 차슈 덩밥을 먹고 싶어서 상무지구의 차슈 덩밥을 먹으러 가자고 말했습니다',\n",
       " ' SD2000282는 SD2000283이 많이 먹는 것 같으니 차슈 덮밥을 먹고 싶어서 상무지구의 차슈 덮밥을 먹으러 가자고 말했습니다',\n",
       " ' SD2000287은 중학교 1학년 때 동아리로 연극동아리에 가입하게 된 이유는 선생님께서 추천해주셨기 때문이라고 답했습니다',\n",
       " ' 단점으로는 감기와 추위가 심해 밖에서의 활동이 제한될 수 있다고 했습니다',\n",
       " ' 또 대전여중 앞쪽에 두부 두루치기가 있고, 성모병원 앞에도 두부 두루치기가 있다고 말했습니다',\n",
       " ' 또 자신이 선물을 선택할 때는 가성비보다는 자신이 좋아하는 것을 선물한다고 했습니다',\n",
       " ' 또 초등학교 때 가장 친했던 친구가 있었는지 물어봤으며, 중학교 때 연극동아리를 하면서 느낀 장점과 단점을 질문했습니다',\n",
       " ' 또한 혈액형은 성격과 관련이 없다는 것을 알고 있어서 혈액형에 의존하여 성격을 판단하지 않는다고 말했습니다',\n",
       " ' 여행 방식은 즉흥적인 여행을 선호한다고 말했습니다'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeated_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exact_repeated_sentences는 가징 첫 번째에 등장한 문장 1개로 통일\n",
    "# + 이런 경우 comsos2에서 등장한 문장 중에서 cosmos25에 등장한 문장과 유사도가 낮은문장을 추가로 가져와 이어붙임\n",
    "# 그 이유는 cosmos25에서 반복되는 문장 때문에 완전한 요약을 아직 못했을 수 있기 때문\n",
    "\n",
    "def remove_repeated_sentences(data:json, exact_repeated_sentences:set, exact_idxes:set) -> None:\n",
    "    \"\"\"\n",
    "    Remove exact repeated sentences from the data.\n",
    "\n",
    "    Parameters:\n",
    "    data (json): Data to remove repeated sentences from.\n",
    "    exact_repeated_sentences (set): Set of exact repeated sentences.\n",
    "    exact_idxes (set): Set of indexes of the exact repeated sentences.\n",
    "    \"\"\"\n",
    "    for idx in sorted(exact_idxes, reverse=True):\n",
    "        output = data[idx]['output']\n",
    "        output_sentences = output.split('.')[:-1]\n",
    "        for sentence in exact_repeated_sentences:\n",
    "            if sentence in output_sentences:\n",
    "                output_sentences.remove(sentence)\n",
    "        data[idx]['output'] = '.'.join(output_sentences) + '.'\n",
    "\n",
    "    with open('filtered_result.json', 'w') as file:\n",
    "        json.dump(data, file, assure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocessing(data:json):\n",
    "    \"\"\"\n",
    "    postprocessing the cosmos_25.json file\n",
    "\n",
    "    < Replace >\n",
    "    [14] -> cosmos2\n",
    "    [15] -> hypernova\n",
    "    [26] -> hypernova\n",
    "    [68] -> hypernova\n",
    "    [98] -> hypernova + '\\n\\n' -> ' '\n",
    "    [105] -> hypernova\n",
    "    [219] -> hypernova\n",
    "    [282] -> hypernova\n",
    "    [365] -> cosmos2\n",
    "\n",
    "\n",
    "    < Unify several into one >\n",
    "    [130] 맨 마지막 3문장 삭제\n",
    "    [166] 맨 마지막 4문장 삭제\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    with open(data, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Replace\n",
    "    data[14]['output'] = cosmos2['output'][14]\n",
    "    data[15]['output'] = hypernova['output'][15]\n",
    "    data[26]['output'] = hypernova['output'][26]\n",
    "    data[68]['output'] = hypernova['output'][68]\n",
    "    data[98]['output'] = hypernova['output'][98].replace('\\n\\n', ' ')\n",
    "    data[105]['output'] = hypernova['output'][105]\n",
    "    data[219]['output'] = hypernova['output'][219]\n",
    "    data[282]['output'] = hypernova['output'][282]\n",
    "    data[365]['output'] = cosmos2['output'][365]\n",
    "\n",
    "    # Unify several into one\n",
    "    data[130]['output'] = '.'.join(data[130]['output'].split('.')[:-4]) + '.'\n",
    "    data[166]['output'] = '.'.join(data[166]['output'].split('.')[:-5]) + '.'\n",
    "\n",
    "\n",
    "    with open('postprocessed_cosmos25.json', 'w') as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocessing('../results/cosmos_25.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 408/408 [00:00<00:00, 859.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of exact repeated sentences found: 11\n",
      "the number of exact repeated sentences found: 6\n",
      "Total number of high similarity repeated sentences found: 22\n",
      "the number of high similarity repeated sentences found: 21\n",
      "Exact repeated sentences found in the following indexes: {328, 269, 23, 216, 221}\n",
      "High similarity repeated sentences found in the following indexes: {129, 261, 137, 396, 14, 149, 169, 302, 310, 316, 328, 204, 206, 336, 349, 356, 103, 360, 111, 369, 125}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 후처리 이후\n",
    "postprocessed_cosmos25 = make_dataframe('./postprocessed_cosmos25.json')\n",
    "repeated_sentences, high_similarity_repeated_sentences, exact_idxes, high_idxes = find_repeated_sentence(postprocessed_cosmos25) # cosmosre12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{23, 216, 221, 269, 328}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_idxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 23\n",
      "Hypernova: 이 대화에서 화자 SD2000064와 SD2000065는 어버이날에 부모님께 주었던 선물과 선물 기준에 대해 이야기했습니다. SD2000064는 부모님께 어버이날과 어머니의 환갑을 기념하기 위해 영상을 만들어서 선물했습니다. 또 어버이날에는 학교나 꽃집에서 판매하는 카네이션을 선물하곤 했는데 이번에는 어머니의 환갑과 어버이날을 겸하여 영상을 선물했습니다. 화자 자신이 영상 편집을 잘 하는 편은 아니지만 어릴 때부터 부모님 결혼식부터 차곡차곡 모아두었던 사진들을 뒤져보며 영상을 만들었는데 감동을 많이 받아 눈물을 흘리기도 했다고 말했습니다. 화자는 선물을 주기 위해 어떤 기준을 세우는지 물어봤는데 화자 자신은 어렸을 때 부모님과 함께 친구 선물을 고를 때 받고 싶은 것을 줘야 한다고 말한 아버지의 말에 따라 자신이 받고 싶은 것들을 선물하는 편이라고 말했습니다. 또 화자는 가격 상관없이 자신이 좋아하는 것을 선물하는 편이지만 웃사 위인 사람들에게는 가격을 고려하여 선물을 주는 편이라고 말했습니다.\n",
      "\n",
      "SD2000065는 부모님에게 어버이날로 초밥을 선물했습니다. 초밥은 예전 아르바이트했던 초밥집의 생선과 실장님의 정성을 다해 만든 초밥이기 때문에 믿고 갔다고 말했습니다. 또 가족끼리 좋은 곳을 가본 적이 없어서 초밥을 선택하게 된 것이라고 말했습니다. 화자에게 어버이 날 선물로 주었는지 물어보고 화자는 어버이 날과 어머니 환갑을 겸하여 카네이션과 영상을 선물했다고 말했습니다. 그리고 화자에게 선물을 주기 위한 기준이 있는지 물어봤고 화자는 어렸을 때 친구 선물 고르러 아버지와 함께 갔을 때 아버지가 받을 수 있는 것을 줘라 하셨기 때문에 자신이 좋아하는 것들을 선물한다고 말했습니다. 가격 상관없이 선물을 주지만 웃사인 사람에게는 가격을 고려해서 선물을 준다며 가격을 생각하게 되는 것 같다고 말했습니다.\n",
      "Cosmos2: 두 화자는 이 대화에서 어버이날 선물과 선물 기준에 대해 이야기했습니다. SD2000064는 어버이날 선물로 어머니의 환갑을 기념하여 영상을 만들었다고 말했습니다. 또 자신이 선물을 선택할 때는 가성비보다는 자신이 좋아하는 것을 선물한다고 했습니다. SD2000065는 어버이날 선물로 부모님과 함께 방문한 비싼 초밥집을 선물했다고 말했습니다. 또 자신이 선물을 선택할 때는 가성비보다는 자신이 좋아하는 것을 선물한다고 했습니다.\n",
      "Cosmos25: 두 화자는 이 대화에서 어버이날 선물과 선물 기준에 대해 이야기했습니다. SD2000064는 어버이날 선물로 어머니의 환갑을 기념하여 영상을 만들었다고 말했습니다. 또 자신이 선물을 선택할 때는 가성비보다는 자신이 좋아하는 것을 선물한다고 했습니다. SD2000065는 어버이날 선물로 부모님과 함께 방문한 비싼 초밥집을 선물했다고 말했습니다. 또 자신이 선물을 선택할 때는 가성비보다는 자신이 좋아하는 것을 선물한다고 했습니다.\n"
     ]
    }
   ],
   "source": [
    "compare_outputs(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' SD2000287은 중학교 1학년 때 동아리로 연극동아리에 가입하게 된 이유는 선생님께서 추천해주셨기 때문이라고 답했습니다',\n",
       " ' 단점으로는 감기와 추위가 심해 밖에서의 활동이 제한될 수 있다고 했습니다',\n",
       " ' 또 자신이 선물을 선택할 때는 가성비보다는 자신이 좋아하는 것을 선물한다고 했습니다',\n",
       " ' 또 초등학교 때 가장 친했던 친구가 있었는지 물어봤으며, 중학교 때 연극동아리를 하면서 느낀 장점과 단점을 질문했습니다',\n",
       " ' 또한 혈액형은 성격과 관련이 없다는 것을 알고 있어서 혈액형에 의존하여 성격을 판단하지 않는다고 말했습니다',\n",
       " ' 여행 방식은 즉흥적인 여행을 선호한다고 말했습니다'}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeated_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {' SD2000058은 가족 중에서 가장 건강에 염려스러운 것은 치매라고 말했습니다': [(14,\n",
       "               ' SD2000058은 가족 중에서 가장 건강에 염려스러운 것은 치매라고 말했습니다',\n",
       "               ' SD2000059는 가족 중에서 가장 건강에 염려스러운 것은 어머니의 고혈압이라고 말했습니다')],\n",
       "             ' 가족과 함께 가는다면 단양을 추천하며, 친구들과 함께 가는다면 어디든 좋을 것 같다고 말했습니다': [(103,\n",
       "               ' 가족과 함께 가는다면 단양을 추천하며, 친구들과 함께 가는다면 어디든 좋을 것 같다고 말했습니다',\n",
       "               ' 가족과 함께 가는다면 어디든 좋을 것 같고, 친구들과 함께 가는다면 어디든 좋을 것 같다고 말했습니다')],\n",
       "             ' SD2000160은 해외 여행지 추천과 국내 여행지 추천을 요청했고, 음식 맛 좋은 곳 추천을 요청했습니다': [(111,\n",
       "               ' SD2000160은 해외 여행지 추천과 국내 여행지 추천을 요청했고, 음식 맛 좋은 곳 추천을 요청했습니다',\n",
       "               ' SD2000161은 해외 여행지 추천과 국내 여행지 추천, 음식 맛 좋은 곳 추천, 관광지 추천을 했습니다'),\n",
       "              (111,\n",
       "               ' SD2000160은 해외 여행지 추천과 국내 여행지 추천을 요청했고, 음식 맛 좋은 곳 추천을 요청했습니다',\n",
       "               ' SD2000160은 해외 여행지 추천을 요청했고, 국내 여행지 추천을 요청했습니다')],\n",
       "             ' SD2002376은 상대방에게 최근에 연애를 했다고 말하며 상대방보다 연애를 빨리 식는 편이고, 상대방보다 연애를 더 오래하는 편이라고 말했습니다': [(125,\n",
       "               ' SD2002376은 상대방에게 최근에 연애를 했다고 말하며 상대방보다 연애를 빨리 식는 편이고, 상대방보다 연애를 더 오래하는 편이라고 말했습니다',\n",
       "               ' SD2002375는 상대방에게 최근에 연애를 했다고 말하며 상대방보다 연애를 오래하는 편이며, 상대방보다 연애를 더 자주 하는 편이라고 말했습니다')],\n",
       "             ' SD2000174는 최근에 본 영화로는 1927를 꼽았으며, 영화관에 혼자 갔을 때 영화가 생각보다 재밌어서 감동받았다고 말했습니다': [(129,\n",
       "               ' SD2000174는 최근에 본 영화로는 1927를 꼽았으며, 영화관에 혼자 갔을 때 영화가 생각보다 재밌어서 감동받았다고 말했습니다',\n",
       "               ' SD2000175는 최근에 본 영화로는 1917를 꼽았으며, 영화관에 혼자 갔을 때 영화가 생각보다 재밌어서 감동받았다고 말했습니다')],\n",
       "             ' 또 돈에 구애받지 않고 갈 수 있는 곳을 꼽으면 스위스를 가장 가고 싶다고 말했고, 유럽의 중립적인 나라로 평화로운 풍경이 인상적이라고 설명했습니다': [(137,\n",
       "               ' 또 돈에 구애받지 않고 갈 수 있는 곳을 꼽으면 스위스를 가장 가고 싶다고 말했고, 유럽의 중립적인 나라로 평화로운 풍경이 인상적이라고 설명했습니다',\n",
       "               ' 또 돈에 구애받지 않고 갈 수 있는 곳을 꼽으면 스페인을 가장 가고 싶다고 말했고, 독일은 계획적인 도시여서 가보고 싶다고 했습니다')],\n",
       "             '두 화자는 이 대화에서 상대방에게 자신이 가장 좋아하는 남자의 유형에 대해 이야기했습니다': [(149,\n",
       "               '두 화자는 이 대화에서 상대방에게 자신이 가장 좋아하는 남자의 유형에 대해 이야기했습니다',\n",
       "               ' SD2002377은 자신의 첫사랑 이야기를 말하면서 상대방에게 자신이 가장 좋아하는 남자의 유형에 대해 이야기했습니다')],\n",
       "             ' SD2002371은 남편과 연애했던 상대와 결혼한 남편의 차이를 설명했습니다': [(169,\n",
       "               ' SD2002371은 남편과 연애했던 상대와 결혼한 남편의 차이를 설명했습니다',\n",
       "               ' SD2002372는 남편과 연애할 때와 결혼한 남편의 차이를 설명했습니다')],\n",
       "             ' SD2000261은 초등학교 6학년 때의 담임 선생님과 고등학교 1학년 때의 담임 선생님을 기억에 남는 선생님이라고 말했습니다': [(204,\n",
       "               ' SD2000261은 초등학교 6학년 때의 담임 선생님과 고등학교 1학년 때의 담임 선생님을 기억에 남는 선생님이라고 말했습니다',\n",
       "               ' SD2000263은 고등학교 1학년 때의 담임 선생님을 기억에 남는 선생님이라고 말했습니다')],\n",
       "             ' 또 인스턴트 식품을 줄이기 위해 노력하고 있으며, 아이들에게 인스턴트 식품을 줄이기 위해 노력하고 있다고 말했습니다': [(206,\n",
       "               ' 또 인스턴트 식품을 줄이기 위해 노력하고 있으며, 아이들에게 인스턴트 식품을 줄이기 위해 노력하고 있다고 말했습니다',\n",
       "               ' 또 인스턴트 식품을 줄이기 위해 무첨가 제품을 선택하려고 노력하고 있다고 말했습니다')],\n",
       "             ' SD2000351은 친한 친구에게는 4~5만원, 친하지 않은 친구에게는 2만원 이하의 선물을 주는 편이라고 말했습니다': [(261,\n",
       "               ' SD2000351은 친한 친구에게는 4~5만원, 친하지 않은 친구에게는 2만원 이하의 선물을 주는 편이라고 말했습니다',\n",
       "               ' SD2000350은 친밀도가 높은 친구에게는 4~5만원, 친하지 않은 친구에게는 2만원 이하의 선물을 주는 편이라고 말했습니다')],\n",
       "             ' SD2000481은 최근에 옻닭을 먹어본 적이 있다고 말하며 옻닭을 좋아한다고 했습니다': [(302,\n",
       "               ' SD2000481은 최근에 옻닭을 먹어본 적이 있다고 말하며 옻닭을 좋아한다고 했습니다',\n",
       "               ' SD2000482는 최근에 백숙을 먹어본 적이 있다고 말하며 백숙을 좋아한다고 했습니다')],\n",
       "             ' 또 자신은 문구사 아르바이트를 추천하고 싶다고 말했습니다': [(310,\n",
       "               ' 또 자신은 문구사 아르바이트를 추천하고 싶다고 말했습니다',\n",
       "               ' 또 카페 아르바이트를 도전해보고 싶다고 말했습니다')],\n",
       "             ' 또한 지인에게 직접 케이크를 만들어 주었을 때 기분이 좋았다고 말하며, 선물을 주는 것이 자신에게도 기분이 좋다고 말했습니다': [(316,\n",
       "               ' 또한 지인에게 직접 케이크를 만들어 주었을 때 기분이 좋았다고 말하며, 선물을 주는 것이 자신에게도 기분이 좋다고 말했습니다',\n",
       "               ' 또한 지인에게 꽃을 선물했을 때 기분이 좋았다고 말하며, 선물 받는 것보다 선물을 주는 것이 자신에게도 기분이 좋다고 말했습니다')],\n",
       "             ' SD2002384는 엠비티아이가 엔프피(EF)인지 엔프제(IJ)인지에 따라 성격이 달라진다고 말했습니다': [(328,\n",
       "               ' SD2002384는 엠비티아이가 엔프피(EF)인지 엔프제(IJ)인지에 따라 성격이 달라진다고 말했습니다',\n",
       "               ' SD2002385는 엠비티아이가 EF인지 IJ인지에 따라 성격이 달라진다고 말했습니다')],\n",
       "             ' SD2002390은 간호사, 유아교육과, 군인, 소방관, 호텔리어, 빵 가게 등 6개의 꿈이 있다고 말했습니다': [(336,\n",
       "               ' SD2002390은 간호사, 유아교육과, 군인, 소방관, 호텔리어, 빵 가게 등 6개의 꿈이 있다고 말했습니다',\n",
       "               ' SD2002391은 유아교육과, 경찰, 군인, 소방관, 호텔리어, 빵 가게 등 6개의 꿈이 있다고 말했습니다')],\n",
       "             ' SD2000544는 어린 시절 가족여행 중 가장 기억에 남는 것은 돌머리 해수욕장에서 장어를 잡았던 것이며, 가족이 함께 텐트를 치면서 함께하는 시간이 좋았다고 말했습니다': [(349,\n",
       "               ' SD2000544는 어린 시절 가족여행 중 가장 기억에 남는 것은 돌머리 해수욕장에서 장어를 잡았던 것이며, 가족이 함께 텐트를 치면서 함께하는 시간이 좋았다고 말했습니다',\n",
       "               ' SD2000545는 가족여행 중 가장 기억에 남는 것은 아버지와 함께 캠프를 떠난 것이며, 가족이 함께 텐트를 치면서 함께하는 시간이 좋았다고 말했습니다')],\n",
       "             ' 특히 평일에는 학교를 가지 않아 혼자 남아서 배달 음식을 많이 시켜 먹으며, 가장 많이 시키는 것은 떡볶이와 치킨이라고 했습니다': [(356,\n",
       "               ' 특히 평일에는 학교를 가지 않아 혼자 남아서 배달 음식을 많이 시켜 먹으며, 가장 많이 시키는 것은 떡볶이와 치킨이라고 했습니다',\n",
       "               ' SD2000552는 평일에는 학교를 가지 않아 배달 음식을 많이 시켜 먹으며, 가장 많이 시키는 것은 애플 음식이라고 말했습니다')],\n",
       "             ' 또한, 반려동물에 대한 사회적 인식이 개선되었으며, 반려동물을 키우는 사람들이 많아졌다고 말했습니다': [(360,\n",
       "               ' 또한, 반려동물에 대한 사회적 인식이 개선되었으며, 반려동물을 키우는 사람들이 많아졌다고 말했습니다',\n",
       "               ' SD2000563은 반려동물에 대한 사회적 인식이 개선되었다고 말하며, 반려동물을 키우는 사람들에게 감탄한다고 말했습니다')],\n",
       "             ' 연애관은 데이트는 꼭 해야 하는 것은 아니며, 상대방이 시키는 것을 억지로 하는 것은 싫다고 했습니다': [(369,\n",
       "               ' 연애관은 데이트는 꼭 해야 하는 것은 아니며, 상대방이 시키는 것을 억지로 하는 것은 싫다고 했습니다',\n",
       "               ' 또한 데이트는 꼭 해야 하는 것은 아니며, 상대방이 시키는 것을 억지로 하는 것은 싫다고 했습니다')],\n",
       "             ' SD2000666은 건강을 위해 보조 식품을 먹고 있다고 말했습니다': [(396,\n",
       "               ' SD2000666은 건강을 위해 보조 식품을 먹고 있다고 말했습니다',\n",
       "               ' 또한 눈 건강을 위해 특별히 챙기는 식품을 먹고 있다고 말했습니다')]})"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_similarity_repeated_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나는 사람이다\n",
      "나는 사람이다.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 주어진 문장들\n",
    "sentences = [\n",
    "    \"나는 사람이다 나는 사람이다 나는 사람이다\",\n",
    "    \"나는 사람이다. 나는 사람이다. 나는 사람이다.\"\n",
    "]\n",
    "\n",
    "# 정규 표현식을 사용하여 중복된 부분 제거\n",
    "def remove_duplicates(sentence):\n",
    "    # 패턴을 통해 문장을 나누고, 중복을 제거\n",
    "    pattern = r'(\\b나는 사람이다\\b[. ]*)'\n",
    "    unique_sentence = ''.join([match.group(1) for i, match in enumerate(re.finditer(pattern, sentence)) if match.group(1) not in sentence[:match.start()]])\n",
    "    return unique_sentence.strip()\n",
    "\n",
    "# 각 문장에 대해 중복 제거 수행\n",
    "results = [remove_duplicates(sentence) for sentence in sentences]\n",
    "\n",
    "# 결과 출력\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-gram을 이용한 반복 문장 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "def find_common_ngrams(df, n, threshold=2):\n",
    "    \"\"\"\n",
    "    Find common n-grams in the 'output' column of the DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    sentences = df['output']\n",
    "\n",
    "    def preprocess(sentence):\n",
    "        return sentence.split()\n",
    "\n",
    "    total_ngrams_dict = dict()\n",
    "    cnt = 0\n",
    "\n",
    "    for idx,sentence in enumerate(sentences):\n",
    "        words = preprocess(sentence)\n",
    "        ngrams_list = list(ngrams(words, n))\n",
    "        ngrams_dict = Counter(ngrams_list)\n",
    "\n",
    "        total_ngrams_dict[idx] = ngrams_dict\n",
    "\n",
    "        if ngrams_dict.most_common(1)[0][1] > 2:\n",
    "            print(f\"Index: {idx}\")\n",
    "            print(f\"Sentence: {sentence}\")\n",
    "            print(f\"Most common {n}-gram: {ngrams_dict.most_common(1)}\")\n",
    "            print(\"\\n\")\n",
    "            cnt+=1\n",
    "    \n",
    "    print(f\"Total number of sentences with common {n}-grams which appear more than {threshold}: {cnt}\")\n",
    "\n",
    "    return total_ngrams_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 14\n",
      "Sentence: 두 화자는 이 대화에서 가족의 건강 상태와 자신의 건강 관리 방식에 대해 이야기했습니다. SD2000058은 가족 중에서 가장 건강에 염려스러운 것은 치매라고 말했습니다. 또한 자신은 현재 건강이 좋지만 치아가 나쁘기 때문에 치아 관리에 대해 질문했습니다. 또 가족 중에서 가장 건강에 염려스러운 것은 치매라고 말했고, 가족 중에서 가장 건강에 염려스러운 것은 치매라고 말했습니다. SD2000059는 가족 중에서 가장 건강에 염려스러운 것은 고혈압이라고 말했습니다. 또한 자신은 기립성 저혈압을 가지고 있어 운동을 통해 건강을 증진시키고 있다고 말했습니다. 그리고 가족 중에서 가장 건강에 염려스러운 것은 고혈압이라고 말했습니다.\n",
      "Most common 6-gram: [(('가족', '중에서', '가장', '건강에', '염려스러운', '것은'), 5)]\n",
      "\n",
      "\n",
      "Index: 68\n",
      "Sentence: 두 화자는 이 대화에서 가족과의 관계와 환경에 대해 이야기했습니다. SD2000113은 자신이 어학연수에서 자유롭게 놀다가 귀국 후 가족과의 갈등이 발생했다고 말했습니다. 그리고 가족이 타인으로 대우받는다고 생각하며 가족과의 갈등을 해결하기 위해 타인을 기준으로 생각해보려고 한다며 가족과의 갈등을 해결하려는 방법을 설명했습니다. SD2000114는 가족이 타인으로 대우받는다고 생각하며 가족과의 갈등을 해결하기 위해 가족과의 갈등을 해결하려는 방법을 설명했습니다. 그리고 가족이 타인으로 대우받는다고 생각하며 가족과의 갈등을 해결하기 위해 가족과의 갈등을 해결하려는 방법을 설명했습니다.\n",
      "Most common 6-gram: [(('가족이', '타인으로', '대우받는다고', '생각하며', '가족과의', '갈등을'), 3)]\n",
      "\n",
      "\n",
      "Index: 130\n",
      "Sentence: 두 화자는 이 대화에서 영화와 감독에 대해 이야기했습니다. SD2000174는 상을 받은 영화는 보지 않는 편이라고 말했습니다. 이창동 감독의 영화는 노트북으로 보는데 너무 어려워서 보지 않는다고 했습니다. 하지만 기생충은 쉽게 보급형으로 만들어져 재밌게 봤다며, 영화를 보는데 무슨 느낌인지 구체적으로 말할 수 없어서 실루엣만 알겠는 영화는 별로 좋아하지 않는다고 했습니다. SD2000175는 이창동 감독의 영화는 어려워도 한번 보자 해서 봤는데, 재미가 엄청 재밌다고 한 것도 아니었지만 마지막 장면이 인상에 남았다고 말했습니다. 또한 이창동 감독의 영화는 어려워도 재미가 있고 의미가 있는 영화들이 많아서 인기가 많았을 것 같다고 했습니다. SD2000174는 영화를 보는데 어떤 느낌인지 구체적으로 말할 수 없어서 실루엣만 알겠는 영화는 별로 좋아하지 않는다고 했습니다. SD2000175는 이창동 감독의 영화는 어려워도 재미가 있고 의미가 있는 영화들이 많아서 인기가 많았을 것 같다고 말했습니다. 또한 이창동 감독의 영화는 어려워도 재미가 있고 의미가 있는 영화들이 많아서 인기가 많았을 것 같다고 말했습니다.\n",
      "Most common 6-gram: [(('이창동', '감독의', '영화는', '어려워도', '재미가', '있고'), 3)]\n",
      "\n",
      "\n",
      "Index: 149\n",
      "Sentence: 두 화자는 이 대화에서 상대방에게 자신이 가장 좋아하는 남자의 유형에 대해 이야기했습니다. SD2002378은 남자의 로망이라고 말하며 상대방의 첫사랑 이야기를 듣고 싶다고 말했습니다. SD2002377은 자신의 첫사랑 이야기를 말하면서 상대방에게 자신이 가장 좋아하는 남자의 유형에 대해 이야기했습니다. SD2002377은 중학교 2학년 때 같은 아파트에 사는 사람인 name2와 처음 만났으며, name2는 자신에게 매우 잘 맞는 외모를 가지고 있다고 말했습니다. 그리고 중학교 2학년 때부터 중학교 3학년 때까지 name2와 사귀었으며, 상대방에게 자신이 대시를 많이 해본 여자는 name2라고 말했습니다. 또한, 상대방에게 자신이 가장 좋아하는 남자의 유형에 대해 이야기하고 싶다고 말했습니다.\n",
      "Most common 6-gram: [(('상대방에게', '자신이', '가장', '좋아하는', '남자의', '유형에'), 3)]\n",
      "\n",
      "\n",
      "Index: 219\n",
      "Sentence: 두 화자는 이 대화에서 음식에 대해 이야기했습니다. SD2000282는 오늘 아침에 엄마가 해준 볶음밥이 평소보다 맛이 없었다고 말하면서 오늘 끝나고 먹으러 갈까를 물었습니다. SD2000283은 아빠가 주꾸미 삽겹살을 사올 거라고 하면서 집 가야 한다고 말했고, 다음에 파스타 먹으러 갈까를 물었습니다. SD2000282는 SD2000283에게 주꾸미 볶음을 먹었던 거 있지, 가내수 공업을 가서 먹고 싶다고 말했습니다. SD2000283은 SD2000282가 맨날 먹는 니뽕, 내뽕, 짬뽕을 먹으려 한다며 차슈 덮밥을 먹고 싶다고 말했습니다. SD2000282는 SD2000283이 많이 먹는 것 같으니 차슈 덮밥을 먹고 싶어서 상무지구의 차슈 덮밥을 먹으러 가자고 말했습니다. SD2000283은 SD2000282가 맨날 먹는 짬뽕을 먹으려 한다며 차슈 덮밥을 먹고 싶다고 말했습니다. SD2000282는 SD2000283이 많이 먹는 것 같으니 차슈 덮밥을 먹고 싶어서 상무지구의 차슈 덮밥을 먹으러 가자고 말했습니다. SD2000283은 SD2000282가 맨날 먹는 짬뽕을 먹으려 한다며 차슈 덮밥을 먹고 싶다고 말했습니다. SD2000282는 SD2000283이 많이 먹는 것 같으니 차슈 덩밥을 먹고 싶어서 상무지구의 차슈 덩밥을 먹으러 가자고 말했습니다. SD2000283은 SD2000282가 맨날 먹는 짬뽕을 먹으려 한다며 차슈 덩밥을 먹고 싶다고 말했습니다. SD2000282는 SD2000283이 많이 먹는 것 같으니 차슈 덩밥을 먹고 싶어서 상무지구의 차슈 덩밥을 먹으러 가자고 말했습니다. SD2000283은 SD2000282가 맨날 먹는 짬뽕을 먹으려 한다며 차슈 덩밥을 먹고 싶다고 말했습니다. SD2000282는 SD2000283이 많이 먹는 것 같으니 차슈 덩밥을 먹고 싶어서 상무지구의 차슈 덩밥을 먹으러 가자고 말했습니다. SD2000283은 SD2000282가 맨날 먹는 짬뽕을 먹으려 한다며 차슈 덩밥을 먹고 싶다고 말했습니다. SD2000282는 SD2000283이 많이 먹는 것 같으니 차슈 덩밥을 먹고 싶어서 상무지구의 차슈 덩밥을 먹으러 가자고 말했습니다. SD2000283은 SD2000282가 맨날 먹는 짬뽕을 먹으려 한다며 차슈 덩밥을 먹고 싶다고 말했습니다. SD2000282는 SD2000283이 많이 먹는 것 같으니 차슈 덩밥을 먹고 싶어서 상무지구의 차슈 덩밥을 먹으러 가자고 말했습니다. SD2000283은 SD2000282가 맨날 먹는 짬뽕을 먹으려 한다며 차슈 덩밥을 먹고 싶다고 말했습니다. SD2000282는 SD2000283이 많이 먹는 것 같으니 차슈 덩밥을 먹고 싶어서 상무지구의 차슈 덩밥을 먹으러 가자고 말했습니다. SD2000283은 SD2000282가 맨날 먹는 짬뽕을 먹으려 한다며 차슈 덩밥을 먹고 싶다고 말했습니다. SD2000282는 SD2000283이 많이 먹는 것 같으니 차슈 덩밥을 먹고 싶어서 상무지구의 차슈 덩밥을 먹으러 가자고 말했습니다. SD2000283은 SD2000282가 맨날 먹는 짬뽕을 먹으려 한다며 차슈 덩밥을 먹고 싶다고 말했습니다. SD2000282는 SD2000283이 많이 먹는 것 같으니 차슈 덩밥을 먹고 싶어서 상무지구의 차슈 덩밥을 먹으러 가자\n",
      "Most common 6-gram: [(('먹고', '싶다고', '말했습니다.', 'SD2000282는', 'SD2000283이', '많이'), 10)]\n",
      "\n",
      "\n",
      "Index: 221\n",
      "Sentence: 두 화자는 이 대화에서 동아리 활동, 동창에 대해 이야기했습니다. SD2000288은 SD2000287에게 중학교 1학년 때 연극동아리에 가입하게 된 이유와 함께 고등학교 때 가장 친했던 동창이 누구인지 물어봤습니다. SD2000288은 중학교 1학년 때 동아리로 연극동아리에 가입했다고 답했고, 고등학교 때 가장 친했던 동창은 SD2000287이라고 말했습니다. 또 SD2000287에게 초등학교 때 가장 친했던 친구가 있었는지 물어봤으며, 중학교 때 연극동아리를 하면서 느낀 장점과 단점을 질문했습니다. SD2000287은 중학교 1학년 때 동아리로 연극동아리에 가입하게 된 이유는 선생님께서 추천해주셨기 때문이라고 답했습니다. 또한 고등학교 때 가장 친했던 동창은 SD2000288이고, 가장 기억에 남는 친구는 SD2000288이며, 가장 기억에 남는 선생님은 SD2000288이라고 말했습니다. 또 초등학교 때 가장 친했던 친구가 있었는지 물어봤으며, 중학교 때 연극동아리를 하면서 느낀 장점과 단점을 질문했습니다. SD2000287은 중학교 1학년 때 동아리로 연극동아리에 가입하게 된 이유는 선생님께서 추천해주셨기 때문이라고 답했습니다. 또한 고등학교 때 가장 친했던 동창은 SD2000288이며, 가장 기억에 남는 친구는 SD2000288이고, 가장 기억에 남는 선생님은 SD2000288이라고 말했습니다. 또 초등학교 때 가장 친했던 친구가 있었는지 물어봤으며, 중학교 때 연극동아리를 하면서 느낀 장점과 단점을 질문했습니다.\n",
      "Most common 6-gram: [(('초등학교', '때', '가장', '친했던', '친구가', '있었는지'), 3)]\n",
      "\n",
      "\n",
      "Index: 336\n",
      "Sentence: 두 화자는 이 대화에서 꿈에 대해 이야기했습니다. SD2002390은 간호사, 유아교육과, 군인, 소방관, 호텔리어, 빵 가게 등 6개의 꿈이 있다고 말했습니다. SD2002391은 유아교육과, 경찰, 군인, 소방관, 호텔리어, 빵 가게 등 6개의 꿈이 있다고 말했습니다. SD2002390은 간호사, 유아교육과는 추천하고 싶지 않으며, 군인은 공부와 체력이 중요하지만 자신은 공부도 잘하고 운동도 잘하기 어려워 고민이라고 말했습니다. 또한, 군인이 안 되면 소방관, 호텔리어, 빵 가게를 차리는 것 등을 차례로 도전해보려고 한다며 공부는 필수라고 말했습니다. SD2002391은 유아교육과는 좋아하지만 아이를 좋아하지 않아서 추천하고 싶지는 않으며, 경찰, 군인, 소방관, 호텔리어, 빵 가게 등 다른 꿈을 찾고 있다고 말했습니다.\n",
      "Most common 6-gram: [(('군인,', '소방관,', '호텔리어,', '빵', '가게', '등'), 3)]\n",
      "\n",
      "\n",
      "Index: 365\n",
      "Sentence: 두 화자는 이 대화에서 가보고 싶은 여행지, 가보고 싶은 해외 여행지, 가보고 싶은 휴양지 등에 대해 이야기했습니다. SD2000568은 국내 여행지로는 서울이나 자연경관을 볼 수 있는 곳을 추천했고, 해외 여행지로는 일본, 터키, 일본을 추천했습니다. 가족과 함께 가보고 싶은 휴양지는 일본이고, 가보고 싶은 해외 여행지는 남도, 남해, 제주도, 전라도, 일본, 터키, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공, 인도, 중국, 베이징, 상하이, 미국, 멕시코, 브라질, 남아공\n",
      "Most common 6-gram: [(('중국,', '베이징,', '상하이,', '미국,', '멕시코,', '브라질,'), 34)]\n",
      "\n",
      "\n",
      "Total number of sentences with common 6-grams which appear more than 3: 8\n"
     ]
    }
   ],
   "source": [
    "total_ngrams_dict= find_common_ngrams(cosmos25, 6, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
