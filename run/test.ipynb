{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# input length test\n",
    "\n",
    "> input length 512 이하의 행이 몇 개나 있는지?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from transformers import AutoTokenizer, BartForConditionalGeneration\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "model_name = \"alaggung/bart-r3f\"\n",
    "max_length = 512\n",
    "num_beams = 10\n",
    "length_penalty = 1.2\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir='../cache')\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name, cache_dir='../cache')\n",
    "model.eval()\n",
    "\n",
    "# 'inputs', 'labels', 'preds', 'rouge', 'bleu'\n",
    "df = pd.DataFrame()\n",
    "\n",
    "def make_data(path):\n",
    "    inputs = []\n",
    "    labels = []\n",
    "\n",
    "    with open(path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    def make_chat(inp):\n",
    "        for cvt in inp['conversation']:\n",
    "            chats = []\n",
    "            speaker = cvt['speaker']\n",
    "            utterance = cvt['utterance']\n",
    "            chats.append(f\"화자{speaker}: {utterance}\")\n",
    "        \n",
    "        chat = \"[BOS]\" + \"[SEP]\".join(chats)  + \"[EOS]\"\n",
    "        return chat\n",
    "\n",
    "    for example in data:\n",
    "        chat = make_chat(example[\"input\"])\n",
    "        \n",
    "        input = tokenizer(chat, return_tensors=\"pt\")\n",
    "        if input.input_ids.size(1) > 512:\n",
    "            continue\n",
    "\n",
    "        inputs.append(input)\n",
    "        labels.append(example[\"output\"])\n",
    "\n",
    "    return inputs, labels\n",
    "\n",
    "inputs, labels = make_data(\"/mnt/c/Users/hwyew/Downloads/korean_dialogue/korean_dialog/resource/data/train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hanspell 성능 테스트\n",
    "\n",
    "대화체에서도 맞춤법 교정이 유효한가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /mnt/c/Users/hwyew/Downloads/korean_dialogue/korean_dialog/py-hanspell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "\n",
    "def get_passport_key():\n",
    "    \"\"\"네이버에서 '네이버 맞춤법 검사기' 페이지에서 passportKey를 획득\n",
    "\n",
    "        - 네이버에서 '네이버 맞춤법 검사기'를 띄운 후 \n",
    "        html에서 passportKey를 검색하면 값을 찾을 수 있다.\n",
    "\n",
    "        - 찾은 값을 spell_checker.py 48 line에 적용한다.\n",
    "    \"\"\"\n",
    "\n",
    "    url = \"https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=0&ie=utf8&query=네이버+맞춤법+검사기\"\n",
    "    res = requests.get(url)\n",
    "\n",
    "    html_text = res.text\n",
    "\n",
    "    match = re.search(r'passportKey=([^&\"}]+)', html_text)\n",
    "    if match:\n",
    "        passport_key = match.group(1)\n",
    "        return passport_key\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def fix_spell_checker_py_code(file_path, passportKey):\n",
    "    print(passportKey)\n",
    "    \"\"\"획득한 passportkey를 spell_checker.py파일에 적용\n",
    "    \"\"\"\n",
    "    \n",
    "    pattern = r\"'passportKey': '.*'\"\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as input_file:\n",
    "        content = input_file.read()\n",
    "        modified_content = re.sub(pattern, f\"'passportKey': '{passportKey}'\", content)\n",
    "\n",
    "    with open(file_path, 'w', encoding='utf-8') as output_file:\n",
    "        output_file.write(modified_content)\n",
    "    \n",
    "    return \n",
    "\n",
    "\n",
    "# before run\n",
    "spell_checker_file_path = './hanspell/spell_checker.py'\n",
    "\n",
    "passport_key = get_passport_key()\n",
    "if passport_key:\n",
    "    fix_spell_checker_py_code(spell_checker_file_path, passport_key)\n",
    "else:\n",
    "    print(\"passportKey를 찾을 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hanspell import spell_checker\n",
    "\n",
    "result = spell_checker.check(u'나도 인제 안 미루고 맞아는 보려고. 맞아서 소용없으면 소용없는 거고 또 맞아서 좋을 수도 있는 거니까. 그래서 안 걸리면 더 좋은 거고. 가 한번 name3이가 얘기한 대로 혜택 받을 수 있으면 받아가지고 맞아보고.')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.checked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 반복 어구 없애기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 테스트 문자열 리스트\n",
    "test_strings = [\n",
    "    \"name1 is John\",\n",
    "    \"name123\",\n",
    "    \"My name1 is Jane\",\n",
    "    \"name_underscore\",\n",
    "    \"123name\",\n",
    "    \"너는 유산균 아직 안 먹였으면 한번 알아봐가지고 특히 name1이 먼저 먹여. name1이가 밥도 잘 안 먹고 하니까 내가 추천해 주는 거 이로울 만한 거는 없는데 한번 알아봐봐. 있을 거야\"\n",
    "]\n",
    "\n",
    "# 정규표현식 패턴r'\\bname\\S*'\n",
    "pattern = r'name[0-9]\\S*'\n",
    "\n",
    "# 정규표현식을 사용하여 어구 찾기\n",
    "matches = [re.findall(pattern, string) for string in test_strings]\n",
    "\n",
    "# 결과 출력\n",
    "for string, match in zip(test_strings, matches):\n",
    "    print(f\"Original string: {string}\")\n",
    "    print(f\"Matches: {match}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hanspell 제외 전처리 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import koreanize_matplotlib\n",
    "\n",
    "def make_data(data_path):\n",
    "    with open(data_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    text_list = []\n",
    "    def make_chat(inp):\n",
    "        chat = \"\"\n",
    "        for cvt in inp['conversation']:\n",
    "            chat += cvt['utterance']\n",
    "        return chat\n",
    "\n",
    "    for example in data:\n",
    "        text_list.append(make_chat(example['input']))\n",
    "    \n",
    "    text = ' '.join(text_list)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "FONT_PATH = \"/mnt/c/Users/hwyew/Downloads/korean_dialogue/korean_dialog/NanumSquareNeo-cBd.ttf\"\n",
    "\n",
    "def make_cloud(text, title, top_k, max_words=100):\n",
    "    # plt.rc('font', family='NanumGothicCoding')\n",
    "    wordcloud = WordCloud(font_path=FONT_PATH, max_words=max_words, background_color=\"white\").generate(text)\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 1, figsize=(15, 15))\n",
    "\n",
    "    top_data = dict(list(wordcloud.words_.items())[:top_k])\n",
    "    labels, values = zip(*top_data.items())\n",
    "\n",
    "    # 막대 그래프 그리기\n",
    "    axs[0].bar(labels, values)\n",
    "    axs[0].set_xlabel('Labels')\n",
    "    axs[0].set_ylabel('Values')\n",
    "    axs[0].set_title(f\"max count words on {title} data (top {top_k})\", fontsize=36)\n",
    "\n",
    "\n",
    "    # 두 번째 그래프: 워드클라우드\n",
    "    axs[1].imshow(wordcloud)\n",
    "    axs[1].axis('off')\n",
    "    axs[1].set_title(f\"max count words on {title} data (max words = {max_words})\", fontsize=36)\n",
    "\n",
    "    # 그래프 간격 조정\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "## Preprocess functions ##\n",
    "def remove_empty_utterance(data:json):\n",
    "    \"\"\"\n",
    "    Remove empty utterances from the data\n",
    "    \"\"\"\n",
    "    for example in data:\n",
    "        example['input']['conversation'] = [cvt for cvt in example['input']['conversation'] if cvt['utterance'] != '']\n",
    "    return data\n",
    "\n",
    "\n",
    "def correct_wrong_output(data:json):\n",
    "    \"\"\"\n",
    "    Correct wrong speakers in outputs of train samples 'train-000401', 'train-000402'\n",
    "    \"\"\"\n",
    "    data[400]['output'] = data[400]['output'].replace('SD2100504','SD2110504')\n",
    "    data[401]['output'] = data[401]['output'].replace('SD2110503','SD2100503')\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def file_preprocess(data:json):\n",
    "    data = remove_empty_utterance(data)\n",
    "    data = correct_wrong_output(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "불용어 처리\n",
    "- name1, name2..\n",
    "- 뒤에 물결이 붙는 경우 (\"음~\", \"아~\")\n",
    "- 그, 뭐, 어, 인제, 막, 아, 음, 읍, 오, 으\n",
    "- 한 글자가 두번 이상 반복되는 경우 (\"또 또\", \"그 그\")\n",
    "\"\"\"\n",
    "\n",
    "stopwords_pattern = [r'name[0-9]\\S*', r'\\w~', r'\\b으\\b', r'\\b그\\b', r'\\b뭐\\b', r'\\b어\\b',  r'\\b인제\\b', r'\\b막\\b', r'\\b아\\b', r'\\b음\\b', r'\\b읍\\b', r'\\b오\\b', r'\\b으\\b']\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    for pattern in stopwords_pattern:\n",
    "        text = re.sub(pattern, '', text)\n",
    "    \n",
    "    # 두 번 이상 반복되는 경우\n",
    "    text = re.sub(r'\\b(\\w)\\s+\\1\\b', r'\\1', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# stopwords + 반복어구 제거\n",
    "def text_preprocess(text):\n",
    "    text = remove_stopwords(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def make_clean_data(data_path):\n",
    "    with open(data_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    data = file_preprocess(data)\n",
    "    \n",
    "    text_list = []\n",
    "    def make_chat(inp):\n",
    "        chat = \"\"\n",
    "        for cvt in inp['conversation']:\n",
    "            chat += cvt['utterance']\n",
    "\n",
    "        return text_preprocess(chat)\n",
    "\n",
    "    for example in tqdm(data):\n",
    "        text_list.append(make_chat(example['input']))\n",
    "    \n",
    "    text = ' '.join(text_list)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "re.sub(r'name[0-9]\\S*', '', \"name1이가 그래서 정말 슬펐어\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# 현재 날짜와 시간 가져오기\n",
    "now = datetime.now()\n",
    "\n",
    "# 날짜와 시간을 문자열로 포맷팅\n",
    "current_time = now.strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "print(current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 화자 1 - 화자 2\n",
    "\n",
    "꼭 넣어줘야 할까? -> ID 대신 <|A|>, <|B|>로 바꿔서 후처리하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 예시 데이터\n",
    "data = [\n",
    "    {   \n",
    "        \"id\" : \"conv-0001\",\n",
    "        \"speaker_ids\" : {\"SD001\": \"value1\", \"SD002\": \"value2\"}\n",
    "    },\n",
    "    {   \n",
    "        \"id\" : \"conv-0002\",\n",
    "        \"speaker_ids\" : {\"SD003\": \"value1\", \"SD004\": \"value2\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "# JSON 파일에 데이터를 한 줄씩 추가하는 함수\n",
    "def save_to_json_file(file_path, data):\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# 파일 경로\n",
    "file_path = 'data_test.json'\n",
    "\n",
    "# 데이터 추가\n",
    "save_to_json_file(file_path, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "# tokenizer special token\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"MLP-KTLim/llama-3-Korean-Bllossom-8B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "# Add special tokens\n",
    "special_tokens_dict = {'additional_special_tokens': ['<|A|>', '<|B|>']}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A, B 치환 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hwyewon/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-07-31 03:54:05.546469: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-31 03:54:05.678581: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-31 03:54:05.727427: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-31 03:54:05.741228: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-31 03:54:05.849218: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-31 03:54:06.863274: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## utils Config File :  config_bllossom.json\n",
      "INFO:tensorflow:Reading checkpoint /home/hwyewon/.cache/huggingface/metrics/bleurt/bleurt-large-512/downloads/extracted/8bfa7908c0a12eb1d407684a0b23baa967ad9775d360bc1603b9295f68c33339/bleurt-large-512.\n",
      "INFO:tensorflow:Config file found, reading.\n",
      "INFO:tensorflow:Will load checkpoint bert_custom\n",
      "INFO:tensorflow:Loads full paths and checks that files exists.\n",
      "INFO:tensorflow:... name:bert_custom\n",
      "INFO:tensorflow:... vocab_file:vocab.txt\n",
      "INFO:tensorflow:... bert_config_file:bert_config.json\n",
      "INFO:tensorflow:... do_lower_case:True\n",
      "INFO:tensorflow:... max_seq_length:512\n",
      "INFO:tensorflow:Creating BLEURT scorer.\n",
      "INFO:tensorflow:Creating WordPiece tokenizer.\n",
      "INFO:tensorflow:WordPiece tokenizer instantiated.\n",
      "INFO:tensorflow:Creating Eager Mode predictor.\n",
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1722365661.046996    1632 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1722365661.053077    1632 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1722365661.053110    1632 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1722365661.055577    1632 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1722365661.055628    1632 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1722365661.055647    1632 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1722365661.281757    1632 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1722365661.281814    1632 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-31 03:54:21.281823: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1722365661.281854    1632 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-31 03:54:21.282163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9516 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    }
   ],
   "source": [
    "# utils.py\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "from transformers import EvalPrediction, AutoTokenizer\n",
    "import torch\n",
    "import random\n",
    "from typing import Dict\n",
    "import argparse\n",
    "\n",
    "config_name = input(\"## input utils config file name ##\\n\")\n",
    "\n",
    "print(\"## utils Config File : \", config_name)\n",
    "# change abspath to run in ipynb file\n",
    "with open(f'/mnt/g/내 드라이브/국립국어원_일상대화요약/korean_dialog/dialogue-summarization/configs/{config_name}', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# with open(f'configs/{config_name}', 'r') as f:\n",
    "#     config = json.load(f)\n",
    "\n",
    "rouge = evaluate.load('rouge')\n",
    "bert_score = evaluate.load('bertscore')\n",
    "bleurt = evaluate.load('bleurt', 'bleurt-large-512', module_type=\"metric\")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "      \"\"\"\n",
    "      Original Trainer may have a memory leak. \n",
    "      This is a workaround to avoid storing too many tensors that are not needed.\n",
    "      \"\"\"\n",
    "      pred_ids = torch.argmax(logits, dim=-1)\n",
    "      return pred_ids, labels\n",
    "\n",
    "def compute_metrics(eval_pred: EvalPrediction):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config[\"arch\"][\"model_id\"])\n",
    "\n",
    "    # compute Rouge-1 F1 score\n",
    "    labels = eval_pred.label_ids # (batch_size, seq_len)\n",
    "    predictions = eval_pred.predictions[0].reshape(labels.shape[0],-1) # (batch_size, seq_len)\n",
    "\n",
    "    # Replace -100 with pad_token_id\n",
    "    mask  = np.where(labels == -100)\n",
    "    labels[mask] = tokenizer.pad_token_id\n",
    "    predictions[mask] = tokenizer.pad_token_id\n",
    "\n",
    "    # Decoding\n",
    "    predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Simple postprocessing\n",
    "    predictions, labels = postprocess_text(predictions, labels)\n",
    "\n",
    "    rouge_scores = rouge.compute(predictions=predictions, references=labels, rouge_types=[\"rouge1\"])\n",
    "    # rouge_scores = rouge.get_scores(predictions, labels, avg=True)\n",
    "    bert_scores = bert_score.compute(predictions=predictions, references=labels, lang=\"ko\")\n",
    "    bleurt_scores = bleurt.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    bertScore = sum(bert_scores['f1']) / len(labels)\n",
    "    bleurtScore = sum(bleurt_scores['scores']) / len(labels)\n",
    "\n",
    "    rouge1 = rouge_scores['rouge1']\n",
    "    total = (bertScore + bleurtScore + rouge1) / 3\n",
    "\n",
    "    return {\"total\" : round(total, 4), \"rouge1\" : round(rouge1, 4), \"BERTScore\" : round(bertScore, 4), \"BLEURT\": round(bleurtScore, 4)}\n",
    "\n",
    "\n",
    "## Preprocess functions ##\n",
    "def remove_empty_utterance(data:json):\n",
    "    \"\"\"\n",
    "    Remove empty utterances from the data\n",
    "    \"\"\"\n",
    "    for example in data:\n",
    "        example['input']['conversation'] = [cvt for cvt in example['input']['conversation'] if cvt['utterance'] != '']\n",
    "    return data\n",
    "\n",
    "\n",
    "def correct_wrong_output(data:json):\n",
    "    \"\"\"\n",
    "    Correct wrong speakers in outputs of train samples 'train-000401', 'train-000402'\n",
    "    \"\"\"\n",
    "    data[400]['output'] = data[400]['output'].replace('SD2100504','SD2110504')\n",
    "    data[401]['output'] = data[401]['output'].replace('SD2110503','SD2100503')\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def file_preprocess(data:json, is_train=False):\n",
    "    data = remove_empty_utterance(data)\n",
    "\n",
    "    if is_train == True:\n",
    "        data = correct_wrong_output(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "불용어 처리\n",
    "\n",
    "## hyperstella2 ##\n",
    "- name1, name2..\n",
    "- 뒤에 물결이 붙는 경우 (\"음~\", \"아~\")\n",
    "- 그, 뭐, 어, 인제, 막, 아, 음, 읍, 오, 으\n",
    "- 한 글자가 두번 이상 반복되는 경우 (\"또 또\", \"그 그\")\n",
    "\n",
    "\n",
    "## nova ##\n",
    "- name 그대로 유지\n",
    "- 뒤에 물결이 붙는 경우 (\"음~\", \"아~\")\n",
    "- 그, 뭐, 어, 인제, 막, 아, 음, 읍, 오, 으\n",
    "- 단어가 두 번 이상 반복되는 경우 제거 ( r'\\b([가-힣a-zA-Z0-9_]+)\\s+\\1\\b')\n",
    "\n",
    "\n",
    "## nova3, hypernova ##\n",
    "- name 그대로 유지\n",
    "- 뒤에 물결이 붙는 경우 (\"음~\", \"아~\")\n",
    "- 그, 뭐, 어, 인제, 막, 아, 음, 읍, 오, 으\n",
    "- 단어가 두 번 이상 반복되는 경우 제거 ( r'\\b([가-힣a-zA-Z0-9_]+)\\s+\\1\\b')\n",
    "- x를 포함한 단어 제거 (r'\\b[가-힣a-zA-Z]*[xX][가-힣a-zA-Z]*\\b')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "stopwords_pattern = [r'\\w~', r'\\b으\\b', r'\\b그\\b', r'\\b뭐\\b', r'\\b어\\b',  r'\\b인제\\b', r'\\b이제\\b', r'\\b막\\b', r'\\b아\\b', r'\\b음\\b', r'\\b읍\\b', r'\\b오\\b', r'\\b으\\b'] # r'name[0-9]\\S*'\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    # 커스텀 불용어 제거\n",
    "    for pattern in stopwords_pattern:\n",
    "        text = re.sub(pattern, '', text)\n",
    "    \n",
    "    # x를 포함한 단어 제거\n",
    "    text = re.sub(r'\\b[가-힣a-zA-Z]*[xX][가-힣a-zA-Z]*\\b', '', text)\n",
    "\n",
    "    # 단어가 두 번 이상 반복되는 경우 -> 1개로\n",
    "    # text = re.sub(r'\\b(\\w)\\s+\\1\\b', r'\\1', text)\n",
    "    text = re.sub(r'\\b([가-힣a-zA-Z0-9_]+)\\s+\\1\\b', r'\\1', text)\n",
    "\n",
    "    # 공백 두 번 이상 연속 -> 1개로\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# stopwords + 반복 어구 제거\n",
    "def text_preprocess(text):\n",
    "    text = remove_stopwords(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def set_seed(config: Dict):\n",
    "    seed = config['seed']\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "# argparse에서 boolean인자 받기\n",
    "def str2bool(v):\n",
    "    if isinstance(v, bool):\n",
    "       return v\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.py\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# JSON 파일에 데이터를 한 줄씩 추가하는 함수\n",
    "def save_to_json_file(file_path, data):\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, fname, tokenizer):\n",
    "        IGNORE_INDEX=-100\n",
    "        self.inp = []\n",
    "        self.label = []\n",
    "\n",
    "        PROMPT = '''당신은 유능한 AI 어시스턴트 입니다. [대화 내용]과 [대화 키워드]를 보고, [요약문]을 생성해주세요.\\n'''\n",
    "\n",
    "        with open(fname, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        if fname.split('/')[-1].split('.')[0].split('_')[1] == \"train\":\n",
    "            data = file_preprocess(data, is_train=True)\n",
    "        else:\n",
    "            data = file_preprocess(data, is_train=False)\n",
    "\n",
    "        ID_FILE = []\n",
    "\n",
    "        def make_chat(id, inp):\n",
    "            chat = [f\"[대화 키워드]\\n{', '.join(inp['subject_keyword'])}에 대한 대화 내용입니다.\\n[대화 내용]\"]\n",
    "            \n",
    "            # json row로 저장\n",
    "            # 먼저 나온 speaker를 A로 할당\n",
    "            id_row = {\"id\" : id, \"speaker_ids\" : {inp['conversation'][0]['speaker'] : \"<|A|>\"}}\n",
    "\n",
    "            for cvt in inp['conversation']:\n",
    "                speaker_idx = cvt['speaker']\n",
    "\n",
    "                # 2번째 발화자 추가 : 뒤에 나온 speaker를 B로 할당\n",
    "                if speaker_idx not in id_row[\"speaker_ids\"].keys():\n",
    "                    id_row[\"speaker_ids\"][speaker_idx] = \"<|B|>\"\n",
    "\n",
    "                utterance = text_preprocess(cvt['utterance'])\n",
    "\n",
    "                # 비어있는 문장 제거\n",
    "                if len(utterance) == 0:\n",
    "                    continue\n",
    "\n",
    "                chat.append(f\"{id_row['speaker_ids'][speaker_idx]}: {utterance}\")\n",
    "                \n",
    "                \n",
    "\n",
    "            chat = \"\\n\".join(chat)\n",
    "\n",
    "            # speaker dict를 json 파일에 저장\n",
    "            ID_FILE.append(id_row)\n",
    "\n",
    "            question = f\"[요약문]\\n\"\n",
    "            chat = chat + \"\\n\\n\" + question\n",
    "\n",
    "            return chat\n",
    "        \n",
    "        for example in data:\n",
    "            chat = make_chat(example[\"id\"], example[\"input\"])\n",
    "            message = [\n",
    "                {\"role\": \"system\", \"content\": PROMPT},\n",
    "                {\"role\": \"user\", \"content\": chat},\n",
    "            ]\n",
    "     \n",
    "            source = tokenizer.apply_chat_template(\n",
    "                message,\n",
    "                add_generation_prompt=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "            target = example[\"output\"]\n",
    "            if target != \"\":\n",
    "                target += tokenizer.eos_token\n",
    "            target = tokenizer(target,\n",
    "                      return_attention_mask=False,\n",
    "                      add_special_tokens=False,\n",
    "                      return_tensors=\"pt\")\n",
    "            target[\"input_ids\"] = target[\"input_ids\"].type(torch.int64)\n",
    "\n",
    "            input_ids = torch.concat((source[0], target[\"input_ids\"][0]))\n",
    "            labels = torch.concat((torch.LongTensor([IGNORE_INDEX] * source[0].shape[0]), target[\"input_ids\"][0]))\n",
    "            self.inp.append(input_ids)\n",
    "            self.label.append(labels)\n",
    "        \n",
    "        save_to_json_file(os.path.join(\"/mnt/g/내 드라이브/국립국어원_일상대화요약/korean_dialog/dialogue-summarization/resource/data/\", f\"ID_{fname.split('/')[-1]}\"), ID_FILE)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inp)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inp[idx]\n",
    "\n",
    "\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, instances):\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            [torch.tensor(ids) for ids in input_ids], batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence([torch.tensor(lbls) for lbls in labels], batch_first=True, padding_value=-100)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"MLP-KTLim/llama-3-Korean-Bllossom-8B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "# Add special tokens\n",
    "special_tokens_dict = {'additional_special_tokens': ['<|A|>', '<|B|>']}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "dataset = CustomDataset(\"/mnt/g/내 드라이브/국립국어원_일상대화요약/korean_dialog/dialogue-summarization/resource/data/일상대화요약_dev.json\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n당신은 유능한 AI 어시스턴트 입니다. [대화 내용]과 [대화 키워드]를 보고, [요약문]을 생성해주세요.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n[대화 키워드]\\n식품, 건강, 검사에 대한 대화 내용입니다.\\n[대화 내용]\\n<|A|>: 근데 건강이 우리만 건강한다고 되는 게 아니잖아. 애들도 건강도 챙겨줘야 되고 그러잖아. 그러니까 너는 애들한테 따로 먹이는 식품 이런 거 있어?\\n<|B|>: 유산 유산균하고 비타민 그런데 애들이 잘 안 먹지. 나도 잘 안 먹는 데 애들이 먹나? 근데 쫓아다니면서 챙겨줄 수도 없고 그런데 유산균은 꼭 먹이라 그러더라고. 유산균 장이 건강해야지 전체적으로 다 좋아진다고 그러더라고.\\n<|B|>: 그래서 될 수 있으면 비타민도 사놓기는 했는데 그거는 넘어가더라도 나는 유산균은 꼭 먹으라고 얘기한다. 유산균이 어떤 사람은 공복에 먹으라 그러고 어떤 사람은 그냥 아무 때나 먹으라고 하는데 모르겠어. 언제 먹는 게 좋은지\\n<|B|>: 그래서 name2이는 또 천식에 아토피까지 있잖아. 진짜 우리 name2이는 건강을 누구보다 진짜 걔는 정말 신경 써가지고 지켜야 되는데 지금은 애들 유산균 먹이는 거에 집중하지 유산균이 젤 좋다고는 하더라고. 피부나 아니면 장내 활동이 좋아야지 감기나 이런 것도 걸렸을 때 빨리 낫는다고.\\n<|B|>: 그래서 근데 어떤 유산균이 또 좋은지 몰라. 가루로 된 걸 먹이는 사람도 있고 알약으로 된 걸 먹이는 사람도 있는데 나는 그냥 가루로 된 거 그리고 광고 많이 나오는 거 나도 거기에 대한 정보가 많이 없어서 그냥 오래된 회사 거? 이런 데 거 먹이고\\n<|B|>: 그래서 애들이 건강해야 나도 편하기는 한데 애들 건강까지 챙겨준다는 것도 어렵고 지금은 유산균만 먹여. 유산균 먹이고 야채 많이 먹이고 그러는데 야채도 잎채소랑 줄기랑 또 다르대. 영양분이 그러니까 뭐를 먹여야지 건강에 더 좋은지 어렵지 다 찾아서 주기가\\n<|B|>: 너는 유산균 아직 안 먹였으면 한번 알아봐가지고 특히 name1이 먼저 먹여. name1이가 밥도 잘 안 먹고 하니까 내가 추천해 주는 거 이로울 만한 거는 없는데 한번 알아봐봐. 있을 거야\\n<|B|>: 애들 먹이는 거 있어?\\n<|A|>:  나도 유산균하고 종합비타민 이런 거 먹이는데\\n<|A|>: name1이는 고3이잖아. 그래서 내가 홍삼 제품을 먹였었어. 근데 어느 날 생리통이 너무 심한 거야. 그래가지고 가서 검사를 했지. 그랬더니 혹이 있대. 근데 홍삼 제품이 여성호르몬이 많잖아.\\n<|A|>: 그러니까 혹에는 여성호르몬 그러니까 홍삼 제품이 안 좋대. 그런다고 나한테 name1이 무슨 약 먹이는 거 없냐고 물어보더라고. 그래서 이거 쪼금 먹였다 했더니 인제는 먹이지 말라고 하더라고. 그래서 솔직히 건강해지려고 홍삼을 먹인 거잖아.\\n<|A|>: 근데 얘는 또 그걸 먹으면 또 안 된다고 하잖아. 그래서 홍삼은 끊고 유산균하고 비타민 이런 거 먹고 있는데 솔직히 이런 거 맞춰서 먹이는 것도 너무 힘들어. 뭐에 안 좋은 거 이거 못 먹이고. 근데 누가 그렇게 될 줄 알았나.\\n<|A|>: 만 저기 여러 명 중에 한 명 나타난 거겠지만 name1이는 또 내 생각에 이런 걸 먹여서 또 얘가 이렇게 혹에 났나? 이런 생각도 들고 나 혼자 자책도 하고 하여튼 그랬는데 건강은 진짜 우리가 한다고 어떻게 할 수 있는 게 아닌 거 같더라고. 그래서\\n<|A|>: 지금은 몸에 좋다는 그런 것도 못 먹여. 고3이라 좀 뭘 좀 먹여주고 싶은데 홍삼 제품 이런 건 전혀 못 먹여. 그래서 사 논 것도 name1이 아빠가 다 먹었어. 그래서 쪼끔 아까워.\\n<|A|>: 너 요새 폐렴 접종 무슨 접종 하는 게 되게 많잖아. 여자애들 자궁암 그것도 경부암도 있고 어른들 맞는 폐구균 이런 것도 엄청 많잖아. 그런 거 맞는 거에 대해서 생각해 봤어?\\n<|B|>: 진작부터 생각해 봤는데 금액이 만만치 않더라고. 한 번 맞는 거 좋은 거 맞아야 된다는데 기간이 있더라고. 그게 또 1년 가는 게 있고 2년 가는 게 있고 6개월 짜리가 있고 거기에 따라 금액도 틀려지고 근데 그거 맞는다고 해서 진짜 폐렴이 안 걸릴까? 이런 의구심도 들고 그래서 맞아야 되나 말아야 되나 그러는데\\n<|B|>: 우리 시어머니랑 남편은 맞았어. 근데 자기 둘만 맞고 나는 쏙 빼놓고 진짜 그럴 때는 내가 의심이 들어서 안 맞고는 있지만 서운하지. 자기들 맞을 때 나도 좀 맞으라고 하든지.\\n<|B|>: 그러니까 어느 병원이 잘하는 곳이 있대. 저렴하면서 name3이가 언제 얘기하더라. 혜택 받아서 맞을 수 있는 병원도 있고 이렇다 그러니까 너도 찾아봐가지고 맞을 수 있으면 맞아.\\n<|B|>: 나도 안 미루고 맞아는 보려고. 맞아서 소용없으면 소용없는 거고 또 맞아서 좋을 수도 있는 거니까. 그래서 안 걸리면 더 좋은 거고. 가 한번 name3이가 얘기한 대로 혜택 받을 수 있으면 받아가지고 맞아보고.\\n<|B|>: 그리고 식구들만 또 그런 거 맞고 다닌다고 그러고 삐져갖고 있을 때가 아니라 나는 내가 알아서 내 건강을 챙겨야 될 거 같아. 나이도 있고 그러니까 근데 진짜 나이 먹으니까 쪼끄만 거 하나에도 서운해지기는 하더라.\\n<|B|>: 이게 그런 게 다 건강하고도 이어지는 거 같은데 너는 요새 붓고 이런 거는 없어?\\n<|A|>:  한참 붓고 그랬는데 요새는 조금 괜찮긴 하더라고 근데 그것도 나이 들어서 그런 건지 어쩐 건지. 근데 내가 그것도 병원에 가서 검사를 해봐야 되나 이런 생각도 들긴 한데 아직 그것 땜에 병원에 가지는 안 했 안 했고 조금 더 부으면 그때 병원에 가볼까 생각 중이야.\\n\\n[요약문]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n두 화자는 이 대화에서 병원 진료와 자녀를 위한 건강식품에 대해 말했습니다. SD2000691은 자녀들에게 유산균과 종합 비타민을 챙겨준다고 말했습니다. 그런데 고등학교 수험생인 딸아이 건강을 위해 홍삼 제품을 먹였더니 생리통이 심해지고 혹이 생겼다고 말했고, 병원에서 홍삼 제품에 여성호르몬이 많으니 복용하지 말라고 했다며 속상했다고 했습니다. 한편 한동안 부종이 생겼었다며 상태가 심해지면 병원 진료를 받을 계획이라고 말했습니다. SD2000692는 자녀들에게 건강에 좋은 채소와 유산균, 비타민 등의 영양제를 준다고 했습니다. 그리고 폐렴 접종은 금액도 비싸고 효능에 대한 확신이 없어서 고민했었는데 조만간 접종을 받는 걸로 마음을 바꿨다고 했습니다.<|eot_id|>'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BART 요약 테스트\n",
    "\n",
    "요약해서 input length를 줄인다면?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kr_dialog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
