{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## utils Config File :  config_yi.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from src.data import CustomDataset, OriginalDataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\"\"\"\n",
    "id | 원본 데이터 | 전처리 데이터 | label | 요약문\n",
    "\"\"\"\n",
    "# dev data result 넣기\n",
    "JSON_PATH = \"results/dev/dev_result_preprocess_yi.json\"\n",
    "DEV_DATA_PATH = \"resource/data/일상대화요약_dev.json\"\n",
    "MODEL_ID = \"hyeogi/Yi-6b-dpo-v0.2\"\n",
    "DETAIL = \"no_ngram\"\n",
    "\n",
    "with open(JSON_PATH, \"r\") as f:\n",
    "    result_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "dataset = OriginalDataset(DEV_DATA_PATH, tokenizer)\n",
    "dataset_preprocessed = CustomDataset(DEV_DATA_PATH, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(dataset[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(dataset_preprocessed[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load('rouge')\n",
    "bert_score = evaluate.load('bertscore')\n",
    "bleurt = evaluate.load('bleurt', 'bleurt-large-512', module_type=\"metric\")\n",
    "\n",
    "\n",
    "def compute_metrics(label, pred):\n",
    "    # Simple postprocessing\n",
    "    pred, label = postprocess_text(pred, label)\n",
    "\n",
    "    rouge_scores = rouge.compute(predictions=[pred], references=[label], rouge_types=[\"rouge1\"])\n",
    "    # rouge_scores = rouge.get_scores(predictions, labels, avg=True)\n",
    "    bertScore = bert_score.compute(predictions=[pred], references=[label], lang=\"ko\")['f1'][0]\n",
    "    bleurtScore = bleurt.compute(predictions=[pred], references=[label])['scores'][0]\n",
    "\n",
    "    rouge1 = rouge_scores['rouge1']\n",
    "\n",
    "    # print(bertScore, bleurtScore, rouge1)\n",
    "    total = (bertScore + bleurtScore + rouge1) / 3\n",
    "\n",
    "    return {\"total\" : round(total, 4), \"rouge1\" : round(rouge1, 4), \"BERTScore\" : round(bertScore, 4), \"BLEURT\": round(bleurtScore, 4)}\n",
    "\n",
    "def postprocess_text(pred, label):\n",
    "    return pred.strip(), label.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data[0]['inference']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data[0]['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics(result_data[0]['inference'], result_data[0]['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "result_df = pd.DataFrame(columns=['id', 'original', 'preprocessed', 'inference', 'label', 'total', 'rouge1', 'BERTScore', 'BLEURT'])\n",
    "\n",
    "for idx in tqdm.tqdm(range(len(result_data))):\n",
    "    metrics = compute_metrics(result_data[idx]['inference'], result_data[idx]['output'])\n",
    "    row = [result_data[idx]['id'],\n",
    "           tokenizer.decode(dataset[idx], skip_special_tokens=True), \n",
    "           tokenizer.decode(dataset_preprocessed[idx], skip_special_tokens=True),\n",
    "           result_data[idx]['inference'],\n",
    "           result_data[idx]['output'],\n",
    "           metrics['total'],\n",
    "           metrics['rouge1'],\n",
    "           metrics['BERTScore'],\n",
    "           metrics['BLEURT']]\n",
    "    result_df.loc[len(result_df)] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.sort_values(by='total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "result_df.to_csv(os.path.join(\"results/dev/\", f\"{MODEL_ID.split('/')[1]}_{DETAIL}_preprocessed_time_{now.strftime('%Y-%m-%d_%H:%M')}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
